<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Daci Blog</title>
    <meta name="description" content="Daci的博客">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.f-orAREn.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.DvBmvMPr.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.Dr5Fq-8O.js">
    <link rel="modulepreload" href="/assets/chunks/framework.BjlC_BXf.js">
    <link rel="modulepreload" href="/assets/ai_rag_rag4.md.D9UiMew5.lean.js">
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="icon" type="image/svg+xml" href="/logo.svg">
    <meta name="theme-color" content="#3eaf7c">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-32e39dcf><!--[--><!--]--><!--[--><span tabindex="-1" data-v-b340c8f7></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-b340c8f7>Skip to content</a><!--]--><!----><header class="VPNav" data-v-32e39dcf data-v-298a4f5a><div class="VPNavBar" data-v-298a4f5a data-v-883acc06><div class="wrapper" data-v-883acc06><div class="container" data-v-883acc06><div class="title" data-v-883acc06><div class="VPNavBarTitle has-sidebar" data-v-883acc06 data-v-64c800ee><a class="title" href="/" data-v-64c800ee><!--[--><!--]--><!--[--><img class="VPImage logo" src="/logo.svg" alt data-v-2662aff2><!--]--><span data-v-64c800ee>Daci</span><!--[--><!--]--></a></div></div><div class="content" data-v-883acc06><div class="content-body" data-v-883acc06><!--[--><!--]--><div class="VPNavBarSearch search" data-v-883acc06><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-883acc06 data-v-0d30a5d7><span id="main-nav-aria-label" class="visually-hidden" data-v-0d30a5d7> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-0d30a5d7 data-v-fa9a032b><!--[--><span data-v-fa9a032b>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ai/" tabindex="0" data-v-0d30a5d7 data-v-fa9a032b><!--[--><span data-v-fa9a032b>AI</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-0d30a5d7 data-v-f1032559><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-f1032559><span class="text" data-v-f1032559><!----><span data-v-f1032559>技术</span><span class="vpi-chevron-down text-icon" data-v-f1032559></span></span></button><div class="menu" data-v-f1032559><div class="VPMenu" data-v-f1032559 data-v-2c3610af><div class="items" data-v-2c3610af><!--[--><!--[--><div class="VPMenuLink" data-v-2c3610af data-v-c8d6d8c5><a class="VPLink link" href="/tech/frontend/" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>前端</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-2c3610af data-v-c8d6d8c5><a class="VPLink link" href="/tech/backend/" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>后端</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/tools/" tabindex="0" data-v-0d30a5d7 data-v-fa9a032b><!--[--><span data-v-fa9a032b>工具</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-0d30a5d7 data-v-f1032559><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-f1032559><span class="text" data-v-f1032559><!----><span data-v-f1032559>关于</span><span class="vpi-chevron-down text-icon" data-v-f1032559></span></span></button><div class="menu" data-v-f1032559><div class="VPMenu" data-v-f1032559 data-v-2c3610af><div class="items" data-v-2c3610af><!--[--><!--[--><div class="VPMenuLink" data-v-2c3610af data-v-c8d6d8c5><a class="VPLink link" href="/about/fitness/" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>健身</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-2c3610af data-v-c8d6d8c5><a class="VPLink link" href="/about/life/" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>生活</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-883acc06 data-v-aba76a3d data-v-f1032559><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Change language" data-v-f1032559><span class="text" data-v-f1032559><span class="vpi-languages option-icon" data-v-f1032559></span><!----><span class="vpi-chevron-down text-icon" data-v-f1032559></span></span></button><div class="menu" data-v-f1032559><div class="VPMenu" data-v-f1032559 data-v-2c3610af><!----><!--[--><!--[--><div class="items" data-v-aba76a3d><p class="title" data-v-aba76a3d>简体中文</p><!--[--><div class="VPMenuLink" data-v-aba76a3d data-v-c8d6d8c5><a class="VPLink link" href="/en/ai/rag/rag4.html" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>English</span><!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-883acc06 data-v-be3201b0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-be3201b0 data-v-23920a8f data-v-222e4e54><span class="check" data-v-222e4e54><span class="icon" data-v-222e4e54><!--[--><span class="vpi-sun sun" data-v-23920a8f></span><span class="vpi-moon moon" data-v-23920a8f></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-883acc06 data-v-ab7c40f2 data-v-5480e48c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/zhangdanqing/zhangdanqing.github.io" aria-label="github" target="_blank" rel="noopener" data-v-5480e48c data-v-1dcf205f><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-883acc06 data-v-b0498f15 data-v-f1032559><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-f1032559><span class="vpi-more-horizontal icon" data-v-f1032559></span></button><div class="menu" data-v-f1032559><div class="VPMenu" data-v-f1032559 data-v-2c3610af><!----><!--[--><!--[--><div class="group translations" data-v-b0498f15><p class="trans-title" data-v-b0498f15>简体中文</p><!--[--><div class="VPMenuLink" data-v-b0498f15 data-v-c8d6d8c5><a class="VPLink link" href="/en/ai/rag/rag4.html" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>English</span><!--]--></a></div><!--]--></div><div class="group" data-v-b0498f15><div class="item appearance" data-v-b0498f15><p class="label" data-v-b0498f15>Appearance</p><div class="appearance-action" data-v-b0498f15><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-b0498f15 data-v-23920a8f data-v-222e4e54><span class="check" data-v-222e4e54><span class="icon" data-v-222e4e54><!--[--><span class="vpi-sun sun" data-v-23920a8f></span><span class="vpi-moon moon" data-v-23920a8f></span><!--]--></span></span></button></div></div></div><div class="group" data-v-b0498f15><div class="item social-links" data-v-b0498f15><div class="VPSocialLinks social-links-list" data-v-b0498f15 data-v-5480e48c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/zhangdanqing/zhangdanqing.github.io" aria-label="github" target="_blank" rel="noopener" data-v-5480e48c data-v-1dcf205f><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-883acc06 data-v-41ff4f54><span class="container" data-v-41ff4f54><span class="top" data-v-41ff4f54></span><span class="middle" data-v-41ff4f54></span><span class="bottom" data-v-41ff4f54></span></span></button></div></div></div></div><div class="divider" data-v-883acc06><div class="divider-line" data-v-883acc06></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-32e39dcf data-v-14b7a3aa><div class="container" data-v-14b7a3aa><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-14b7a3aa><span class="vpi-align-left menu-icon" data-v-14b7a3aa></span><span class="menu-text" data-v-14b7a3aa>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-14b7a3aa data-v-cd011971><button data-v-cd011971>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-32e39dcf data-v-7f452ddf><div class="curtain" data-v-7f452ddf></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-7f452ddf><span class="visually-hidden" id="sidebar-aria-label" data-v-7f452ddf> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-a7d3a93e><section class="VPSidebarItem level-0 has-active" data-v-a7d3a93e data-v-48f9c2c2><div class="item" role="button" tabindex="0" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><h2 class="text" data-v-48f9c2c2>AI</h2><!----></div><div class="items" data-v-48f9c2c2><!--[--><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/ai/" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>AI 导览</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/ai/cursor.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>Cursor中AI的应用</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/ai/windsurf.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>深度探索Windsurf编辑器中AI的应用</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/ai/rag/rag1.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>解析RAG</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/ai/rag/rag2.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>大模型应用框架解析：RAG、Agent、微调、提示词工程</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/ai/rag/rag3.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>快速搭建支持多模态文档的 RAG 问答系统</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/ai/rag/rag4.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>RAG 文本分块，从策略到优化</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-32e39dcf data-v-82c0ecc6><div class="VPDoc has-sidebar has-aside" data-v-82c0ecc6 data-v-06ef2308><!--[--><!--]--><div class="container" data-v-06ef2308><div class="aside" data-v-06ef2308><div class="aside-curtain" data-v-06ef2308></div><div class="aside-container" data-v-06ef2308><div class="aside-content" data-v-06ef2308><div class="VPDocAside" data-v-06ef2308 data-v-75b22be8><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-75b22be8 data-v-a2152973><div class="content" data-v-a2152973><div class="outline-marker" data-v-a2152973></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a2152973>On this page</div><ul class="VPDocOutlineItem root" data-v-a2152973 data-v-80416021><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-75b22be8></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-06ef2308><div class="content-container" data-v-06ef2308><!--[--><!--]--><main class="main" data-v-06ef2308><div style="position:relative;" class="vp-doc _ai_rag_rag4" data-v-06ef2308><div><h2 id="摘要" tabindex="-1">摘要 <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;摘要&quot;">​</a></h2><p>检索增强生成（RAG）的性能在很大程度上取决于其检索模块的质量，而文本分块（Chunking）是决定检索质量的关键前置步骤。粗暴或不恰当的分块会导致信息丢失、上下文割裂、检索效率低下等问题，严重影响 RAG 系统的最终表现。本文系统性地探讨了文本分块的核心目标与挑战，从克服上下文窗口限制、提高检索精度、维护上下文完整性等角度阐述了其重要性。接着，详细介绍了固定大小、基于句子、递归字符、基于文档结构等基础分块策略，并给出了结合 Markdown 文档结构的混合分块策略的 Python 实现。此外，文章还展望了语义分块、分层分块、句子窗口检索等高级策略，为读者提供了更广阔的技术视野。掌握精妙的文本分块技术，是告别“粗暴切分”，打造高效、精准 RAG 应用的关键一步。</p><h2 id="前言" tabindex="-1">前言 <a class="header-anchor" href="#前言" aria-label="Permalink to &quot;前言&quot;">​</a></h2><p>检索增强生成（RAG）技术通过结合外部知识库与大语言模型（LLM）的生成能力，极大地提升了模型回答问题的准确性和时效性。这项技术已成为构建智能问答、内容生成等应用的热门选择。然而，光鲜亮丽的 RAG 应用背后，一个常常被忽视但至关重要的环节是——如何将庞大的原始文档<strong>切分</strong>成适合检索的“<strong>块</strong>”（Chunks）。 许多开发者在构建 RAG 系统时，可能会图省事，采用最简单直接的方法，比如按固定字数或段落进行“一刀切”。这种看似便捷的“粗暴”分块方式，却往往是导致 RAG 系统性能不佳、表现“智障”的“罪魁祸首”。想象一下：</p><ol><li><strong>上下文“腰斩”</strong>：一个完整的逻辑链条、一段重要的代码示例，或者一个关键的定义，被无情地从中间切开。检索到的单个分块信息残缺，LLM 如同读到半句话，自然难以给出完整准确的答案。</li><li><strong>关键信息“沉底”</strong>：重要的信息恰好散落在多个分块的边缘，导致检索时要么只捞起片段，要么干脆被忽略，如同大海捞针。</li><li><strong>检索“迷失方向”</strong>：分块过大，包含太多噪音，降低了匹配精度；分块过小，语义信息不足，同样影响检索效果。这都会让检索过程效率低下，返回一堆“看似相关，实则无用”的结果。</li></ol><p>因此，选择和设计合适的文本分块策略，绝非小事一桩。它如同为 RAG 系统打造信息高速公路前的地基工程，直接关系到后续检索器能否精准、高效地定位信息，进而决定了 LLM 能否基于高质量的上下文生成令人满意的答案。</p><h2 id="一、为何-rag-离不开文本分块" tabindex="-1">一、为何 RAG 离不开文本分块？ <a class="header-anchor" href="#一、为何-rag-离不开文本分块" aria-label="Permalink to &quot;一、为何 RAG 离不开文本分块？&quot;">​</a></h2><p>在我们深入探讨各种分块策略之前，必须先搞清楚：为什么文本分块在 RAG 架构中如此不可或缺？它究竟解决了什么核心问题？</p><h3 id="_1-1-定义与核心目标" tabindex="-1">1.1 定义与核心目标 <a class="header-anchor" href="#_1-1-定义与核心目标" aria-label="Permalink to &quot;1.1 定义与核心目标&quot;">​</a></h3><p><strong>文本分块（Text Chunking / Splitting）</strong>，顾名思义，就是将原始的、可能非常庞大的文本资料（例如，一篇长篇报告、一本电子书、一个复杂的网页或者大量的 API 文档）分割成一系列更小、更易于处理的文本片段（Chunks）的过程。这些 Chunks 是 RAG 系统中信息处理的基本单元，它们将被送入 <a href="https://zhida.zhihu.com/search?content_id=256772760&amp;content_type=Article&amp;match_order=1&amp;q=Embedding+%E6%A8%A1%E5%9E%8B&amp;zhida_source=entity" target="_blank" rel="noreferrer">Embedding 模型</a>进行向量化，然后存入<a href="https://zhida.zhihu.com/search?content_id=256772760&amp;content_type=Article&amp;match_order=1&amp;q=%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93&amp;zhida_source=entity" target="_blank" rel="noreferrer">向量数据库</a>进行索引，最终服务于检索环节。 文本分块的核心目标，可以归纳为以下几点：</p><ol><li><strong>克服上下文窗口限制</strong> (Overcoming Context Window Limitations)：这是最直接的原因。目前所有的大语言模型（LLM），无论是 GPT 系列、Claude 还是 Llama，都有其能够一次性处理的上下文长度限制（Context Window）。原始文档往往远超这个限制（几千到几十万 Tokens 不等）。如果不进行分块，根本无法将完整的长文档信息有效地提供给 LLM。分块确保了输入给 LLM 的每一段信息都在其“消化能力”范围之内。</li><li><strong>提高检索精度与效率</strong>(Improving Retrieval Accuracy and Efficiency)：</li></ol><ul><li><strong>精度</strong>：想象一下，如果你的查询是“什么是 Transformer 的自注意力机制？”，一个精确包含该定义的、几百字的 Chunk 显然比一个包含整个 Transformer 论文（数千字）的 Chunk 更容易被检索算法（如向量相似度计算）精准命中。大块内容中无关信息多，会稀释相关信号，干扰相似度判断。<strong>小而美、语义集中的块更容易实现精确匹配</strong>。</li><li><strong>效率</strong>：对大量小块文本进行向量化和索引，构建向量数据库，其检索速度通常远快于在整个原始文档集合中进行全文搜索。分块相当于对信息进行了有效的预处理和“分而治之”，使得向量数据库能够快速定位到最可能相关的几个信息片段，极大地提升了检索响应速度。</li></ul><ol><li><strong>维护上下文完整性 (Maintaining Contextual Integrity)：虽然我们要切分，但理想的分块策略并非随意切割。好的分块应该尽可能地保持语义的连贯性和上下文的完整性</strong>。例如，不应在一个句子的中间断开，不应将一个完整的代码块或列表项拆散。目标是让每个 Chunk 本身就能携带相对完整的、有意义的信息单元。</li></ol><h3 id="_1-2-对-rag-性能的深远影响" tabindex="-1">1.2 对 RAG 性能的深远影响 <a class="header-anchor" href="#_1-2-对-rag-性能的深远影响" aria-label="Permalink to &quot;1.2 对 RAG 性能的深远影响&quot;">​</a></h3><p>理解了分块的目标，我们就能明白它如何直接且深刻地影响 RAG 系统的最终性能。分块策略的选择，就像是给 RAG 系统配备了不同规格的“信息管道”，直接决定了流经其中的信息质量和效率，最终体现在两个核心环节：</p><ul><li><p><strong>检索质量 (Retrieval Quality)：这是最直接的影响。分块的粒度（大小）、内容边界的确定方式（按句子？按段落？按固定长度？）、以及块与块之间的关联方式（是否有重叠？），都直接决定了检索器（Retriever）能否在用户提出查询时，准确、全面地</strong>找到最相关的文本片段。糟糕的分块会导致检索器返回：</p></li><li><p><strong>不相关信息</strong>：块太大，包含太多噪音。</p></li><li><p><strong>不完整信息</strong>：块太小，或切割不当，丢失关键上下文。</p></li><li><p><strong>冗余信息</strong>：多个块包含高度重叠或相似的内容。</p></li><li><p><strong>生成质量 (Generation Quality)：RAG 的核心优势在于“检索增强生成”。<a href="https://zhida.zhihu.com/search?content_id=256772760&amp;content_type=Article&amp;match_order=1&amp;q=%E7%94%9F%E6%88%90%E5%99%A8&amp;zhida_source=entity" target="_blank" rel="noreferrer">生成器</a>（Generator，即 LLM）的输出质量高度依赖于检索器提供的上下文信息（检索到的 Chunks）。如果检索到的 Chunks 本身质量就不高（例如，上下文割裂、信息缺失、关键点遗漏），那么即使 LLM 本身能力再强，也如同“巧妇难为无米之炊”，难以生成准确、流畅、逻辑连贯、令人满意的答案。高质量的检索是高质量生成的前提，而恰当的分块是高质量检索的基础</strong>。</p></li></ul><h3 id="_1-3-内在关联与核心挑战-精度-vs-上下文" tabindex="-1">1.3 内在关联与核心挑战：精度 vs. 上下文 <a class="header-anchor" href="#_1-3-内在关联与核心挑战-精度-vs-上下文" aria-label="Permalink to &quot;1.3 内在关联与核心挑战：精度 vs. 上下文&quot;">​</a></h3><p>文本分块并非没有挑战，其核心在于一个经典的权衡（Trade-off）：<strong>检索精度（Precision）</strong> 与 <strong>上下文完整性（Context）</strong> 之间的平衡。</p><ul><li><p><strong>小块</strong>(Smaller Chunks)：</p></li><li><p><strong>优点 ：信息更聚焦，语义更集中。这使得查询向量更容易匹配到包含特定关键词或高度相关语义的小块，从而提高检索的精度</strong>。更容易命中“靶心”。</p></li><li><p><strong>缺点 ：可能丢失重要的上下文</strong>信息。单个小块可能只包含一个事实片段，缺乏必要的背景、前提或后续解释，导致 LLM 无法理解完整的语境或回答需要综合信息的复杂问题。如同盲人摸象，只得其一隅。</p></li><li><p><strong>大块</strong> (Larger Chunks)：</p></li><li><p><strong>优点 ：能保留更丰富的上下文</strong>信息，包含更长的逻辑链条或更完整的背景描述。这有助于 LLM 理解更复杂的概念、事件的前因后果或不同信息点之间的关系。</p></li><li><p><strong>缺点 ：可能包含较多与当前查询不直接相关的信息（噪音），从而稀释了核心相关性信号，可能降低检索精度</strong>（匹配到包含相关词但整体主题跑偏的块）。同时，更大的块也增加了 LLM 处理的负担和潜在的幻觉风险。</p></li></ul><p>因此，<strong>选择或设计分块策略的关键在于，如何根据你的数据特性（文本类型、结构复杂度、信息密度等）和具体的应用场景（问答、摘要、对话等），在这个“精度 vs. 上下文”上找到那个“最优解”</strong>。没有一种分块策略是万能的，理解这个核心挑战是后续选择和优化策略的基础。</p><h2 id="二、基础分块策略" tabindex="-1">二、基础分块策略 <a class="header-anchor" href="#二、基础分块策略" aria-label="Permalink to &quot;二、基础分块策略&quot;">​</a></h2><p>掌握一些基础且常用的分块策略是构建 RAG 系统的起点。这些策略各有优劣，适用于不同的场景。</p><h3 id="_2-1-固定大小分块-fixed-size-chunking" tabindex="-1">2.1 固定大小分块 (Fixed-size Chunking) <a class="header-anchor" href="#_2-1-固定大小分块-fixed-size-chunking" aria-label="Permalink to &quot;2.1 固定大小分块 (Fixed-size Chunking)&quot;">​</a></h3><p>这是最简单、最“懒人”的方法。直接按照固定的字符数（Character Count）或 Token 数（Token Count）来切割文本。为了缓解在边界处强行切断语义的问题，通常会设置一个“重叠”（Overlap）大小。重叠部分意味着每个块的末尾会与下一个块的开头有一段重复的内容。</p><ul><li><p><strong>核心思想</strong>：设定一个 <code>chunk_size</code>（如 500 个字符）和一个 <code>chunk_overlap</code>（如 50 个字符）。从文本开头取 <code>chunk_size</code> 个字符作为第一个块，然后下一次从 <code>start_index + chunk_size - chunk_overlap</code> 的位置开始取下一个块，依此类推。</p></li><li><p><strong>优点</strong> ：</p></li><li><p>实现极其简单，几乎不需要复杂的逻辑。</p></li><li><p>计算开销非常小，处理速度快。</p></li><li><p>对文本格式没有特殊要求。</p></li><li><p><strong>缺点</strong> ：</p></li><li><p><strong>极易破坏语义完整性</strong>：非常可能在句子中间、单词中间（如果按字符切）、代码行中间等不恰当的地方断开，导致上下文严重割裂。</p></li><li><p><strong>忽略文本结构</strong>：完全无视段落、标题、列表等任何文本固有结构。固定大小对于信息密度不同、语言不同的文本效果可能差异巨大。同样的 500 字符，在信息密集的文本中可能只包含半个观点，在稀疏文本中可能包含好几个。</p></li><li><p><strong>适用场景</strong>：</p></li><li><p>对文本结构要求不高的简单场景。</p></li><li><p>数据量极大，需要快速进行初步处理时。</p></li><li><p>作为更复杂分块策略（如递归分块）的最后“兜底”手段。</p></li><li><p>对上下文完整性要求不高的检索任务。</p></li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 概念性示例 (非直接运行代码)</span></span>
<span class="line"><span>def fixed_size_chunking(text, chunk_size, chunk_overlap):</span></span>
<span class="line"><span> &amp;nbsp;  chunks = []</span></span>
<span class="line"><span> &amp;nbsp;  start_index = 0</span></span>
<span class="line"><span> &amp;nbsp;  while start_index &lt; len(text):</span></span>
<span class="line"><span> &amp;nbsp; &amp;nbsp; &amp;nbsp;  end_index = start_index + chunk_size</span></span>
<span class="line"><span> &amp;nbsp; &amp;nbsp; &amp;nbsp;  chunks.append(text[start_index:end_index])</span></span>
<span class="line"><span> &amp;nbsp; &amp;nbsp; &amp;nbsp;  start_index += chunk_size - chunk_overlap</span></span>
<span class="line"><span> &amp;nbsp; &amp;nbsp; &amp;nbsp;  if start_index &gt;= len(text): # 避免因 overlap 超出</span></span>
<span class="line"><span> &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; break</span></span>
<span class="line"><span> &amp;nbsp;  return chunks</span></span>
<span class="line"><span>​</span></span>
<span class="line"><span># 假设 text 是你的长文本</span></span>
<span class="line"><span># chunks = fixed_size_chunking(text, 500, 50)</span></span></code></pre></div><h3 id="_2-2-基于句子的分块-sentence-splitting" tabindex="-1">2.2 基于句子的分块 (Sentence Splitting) <a class="header-anchor" href="#_2-2-基于句子的分块-sentence-splitting" aria-label="Permalink to &quot;2.2 基于句子的分块 (Sentence Splitting)&quot;">​</a></h3><p>这种策略试图尊重语言的自然边界——句子。它首先使用句子分割算法（如基于标点符号 <code>.?!</code>，或使用 NLP 库如 NLTK, SpaCy）将文本分割成独立的句子，然后将一个或多个连续的句子组合成一个 Chunk，使其大小接近目标范围。</p><ul><li><p><strong>核心思想</strong>：先切分成句子，再合并句子成块。可以简单地每个句子是一个块，也可以设定一个目标块大小，将连续的句子合并，直到接近该大小。同样可以引入句子级别的重叠（如一个块包含第 1-3 句，下一个块包含第 3-5 句）。</p></li><li><p><strong>优点</strong> ：</p></li><li><p><strong>更好地保持语义完整性</strong>：因为句子是表达相对完整意思的基本单位，所以很少会在句子内部断开。</p></li><li><p>比固定大小分块更符合自然语言的结构。</p></li><li><p><strong>缺点</strong> ：</p></li><li><p><strong>句子长度差异大</strong>：有的句子很短，有的很长，导致 Chunk 大小不均匀，可能影响后续处理和检索稳定性。</p></li><li><p>简单的基于标点的分割可能不准确（例如，<code>Mr. Smith</code> 中的 <code>.</code>）。需要更可靠的 NLP 工具。</p></li><li><p>对于代码、列表、或者没有明确句子结构的文本（如 JSON, YAML）效果不佳。</p></li><li><p>跨越多个句子的复杂语义关系可能仍然被切断。</p></li><li><p><strong>适用场景</strong>：</p></li><li><p>处理结构良好、以完整句子为主的文本，如新闻文章、报告、小说等。</p></li><li><p>当保持句子层面的语义完整性比较重要时。</p></li></ul><blockquote><p>注：为简化实现并减少外部库依赖，后续的混合分块示例将不采用基于 NLP 库的句子分割，感兴趣的同学可以试试使用它来改进我们的分块器。</p></blockquote><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 概念性示例 (使用简单的标点分割)</span></span>
<span class="line"><span>import re</span></span>
<span class="line"><span>​</span></span>
<span class="line"><span>def sentence_chunking(text, max_chunk_sentences=3):</span></span>
<span class="line"><span> &amp;nbsp;  sentences = re.split(r&#39;(?&lt;=[.?!])\s+&#39;, text) # 简单按标点分割</span></span>
<span class="line"><span> &amp;nbsp;  sentences = [s for s in sentences if s] # 去除空字符串</span></span>
<span class="line"><span> &amp;nbsp;  chunks = []</span></span>
<span class="line"><span> &amp;nbsp;  current_chunk_sentences = []</span></span>
<span class="line"><span> &amp;nbsp;  for sentence in sentences:</span></span>
<span class="line"><span> &amp;nbsp; &amp;nbsp; &amp;nbsp;  current_chunk_sentences.append(sentence)</span></span>
<span class="line"><span> &amp;nbsp; &amp;nbsp; &amp;nbsp;  if len(current_chunk_sentences) &gt;= max_chunk_sentences:</span></span>
<span class="line"><span> &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;  chunks.append(&quot; &quot;.join(current_chunk_sentences))</span></span>
<span class="line"><span> &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;  current_chunk_sentences = [] # 开始新块</span></span>
<span class="line"><span> &amp;nbsp;  if current_chunk_sentences: # 处理剩余句子</span></span>
<span class="line"><span> &amp;nbsp; &amp;nbsp; &amp;nbsp;  chunks.append(&quot; &quot;.join(current_chunk_sentences))</span></span>
<span class="line"><span> &amp;nbsp;  return chunks</span></span>
<span class="line"><span>​</span></span>
<span class="line"><span># chunks = sentence_chunking(text, 3) # 每块最多包含3个句子</span></span></code></pre></div><h3 id="_2-3-递归字符分块-recursive-character-text-splitting" tabindex="-1">2.3 递归字符分块 (Recursive Character Text Splitting) <a class="header-anchor" href="#_2-3-递归字符分块-recursive-character-text-splitting" aria-label="Permalink to &quot;2.3 递归字符分块 (Recursive Character Text Splitting)&quot;">​</a></h3><p>这是 LangChain 等框架中常用的一种更智能的策略。它试图按一个预设的“分隔符”优先级列表来递归地分割文本。</p><ul><li><p><strong>核心思想</strong>：提供一个分隔符列表，按优先级从高到低尝试分割。例如，优先尝试按 <code>\n\n</code> (段落) 分割，如果分割后的块仍然太大，再尝试按 <code>\n</code> (换行符) 分割，然后按空格 <code>\</code> 分割，最后如果还太大，就按字符<code>&quot;&quot;</code> 分割。目标是在保持较大语义块（如段落）的同时，确保最终块大小不超过限制。</p></li><li><p><strong>优点</strong> ：</p></li><li><p><strong>试图保持语义结构</strong>：优先使用段落、换行等更有意义的分隔符，尽可能维持文本的逻辑结构。</p></li><li><p><strong>比固定大小更智能</strong>：避免在不必要的地方断开。</p></li><li><p><strong>适应性强</strong>：对不同类型的文本结构（段落、列表等）有一定适应性， 是固定大小和句子分割的一种折中和改进。</p></li><li><p><strong>缺点</strong> ：</p></li><li><p>实现相对复杂一些。</p></li><li><p>效果依赖于分隔符列表的选择和优先级顺序。</p></li><li><p>对于没有明显分隔符的密集文本，可能最终退化为按字符分割。</p></li><li><p><strong>适用场景</strong>：</p></li><li><p>通用性较好，适用于多种类型的文本文档。</p></li><li><p>当希望在控制块大小的同时尽可能保留文本结构时。</p></li><li><p>许多 RAG 框架的默认或推荐选项。</p></li></ul><h3 id="_2-4-基于文档结构的分块-document-structure-aware-chunking" tabindex="-1">2.4 基于文档结构的分块 (Document Structure-aware Chunking) <a class="header-anchor" href="#_2-4-基于文档结构的分块-document-structure-aware-chunking" aria-label="Permalink to &quot;2.4 基于文档结构的分块 (Document Structure-aware Chunking)&quot;">​</a></h3><p>这种策略利用文档本身的结构信息进行分割，例如 HTML 的 <code>&lt;div&gt;</code>, <code>&lt;p&gt;</code>, <code>&lt;li&gt;</code> 标签，Markdown 的标题 <code>#</code>, <code>##</code>, 列表 <code>-</code>, <code>*</code>，或者 JSON/YAML 的层级结构。</p><ul><li><p><strong>核心思想</strong>：解析文档的结构树或特定标记，基于这些结构元素来定义 Chunks。例如，每个 <code>&lt;p&gt;</code> 标签内容作为一个 Chunk，或者每个 Markdown 的二级标题下的所有内容作为一个 Chunk。</p></li><li><p><strong>优点</strong> ：</p></li><li><p><strong>高度尊重原文结构</strong>：能够最好地保持作者组织信息的方式。</p></li><li><p><strong>语义连贯性强</strong>：通常一个结构元素（如段落、列表项）包含一个相对独立的语义单元。</p></li><li><p>可以方便地将结构信息（如标题、标签）作为元数据（Metadata）附加到 Chunk 上，这对后续检索很有帮助。</p></li><li><p><strong>缺点</strong> ：</p></li><li><p><strong>依赖于清晰、一致的文档结构</strong>：如果文档结构混乱或没有明确标记，则效果不佳。</p></li><li><p>不同结构元素的文本量可能差异巨大，导致 Chunk 大小极不均匀。例如，一个段落可能很短，另一个章节可能很长。</p></li><li><p>需要针对不同的文档格式（HTML, Markdown, LaTeX, JSON...）编写不同的解析逻辑。</p></li><li><p><strong>适用场景</strong>：</p></li><li><p>处理具有清晰、标准化结构的文档，如网页、Markdown 文档、结构化数据（JSON/XML）等。</p></li><li><p>当需要利用文档结构信息进行检索或过滤时。</p></li></ul><h3 id="_2-5-混合分块-hybrid-chunking" tabindex="-1">2.5 混合分块 (Hybrid Chunking) <a class="header-anchor" href="#_2-5-混合分块-hybrid-chunking" aria-label="Permalink to &quot;2.5 混合分块 (Hybrid Chunking)&quot;">​</a></h3><p>顾名思义，混合策略结合了上述两种或多种方法的优点，试图达到更好的平衡。一个常见的组合是：<strong>首先尝试基于文档结构（如 Markdown 标题）进行高级别分割，然后在这些较大的分割块内部，如果它们仍然超过了目标大小，再使用递归字符分块或句子分块进行细粒度的切分。</strong></p><ul><li><p><strong>核心思想</strong>：分层处理。先用结构化或语义边界（如标题）做粗粒度切分，得到逻辑上相关的“大块”，再对这些“大块”应用更细粒度的策略（如递归字符分割）来满足大小限制，同时保留“大块”的上下文信息（如将标题作为元数据）。</p></li><li><p><strong>优点</strong> ：</p></li><li><p><strong>兼顾结构与大小限制</strong>：既能利用文档的宏观结构，又能确保最终块大小可控。</p></li><li><p><strong>元数据丰富</strong>：可以方便地继承来自结构化分割的元数据（如标题层级）。</p></li><li><p>灵活性高，可根据需求定制组合策略。</p></li><li><p><strong>缺点</strong> ：</p></li><li><p>实现复杂度相对较高。</p></li><li><p>需要仔细设计组合逻辑和参数。</p></li><li><p><strong>适用场景</strong>：</p></li><li><p>对分块质量要求较高，希望在保持上下文、利用结构和控制大小之间取得良好平衡的场景。</p></li><li><p>处理像 Markdown、富文本文档这样既有结构又有自由文本的内容。</p></li></ul><h2 id="三、高级分块策略" tabindex="-1">三、高级分块策略 <a class="header-anchor" href="#三、高级分块策略" aria-label="Permalink to &quot;三、高级分块策略&quot;">​</a></h2><p>当基础策略无法满足更复杂的需求时，或者当你想追求极致的检索效果时，可以探索以下更高级的分块方法。这些方法通常更侧重于<strong>语义理解</strong>或利用<strong>更复杂的模型/流程</strong>。</p><h3 id="_3-1-语义分块-semantic-chunking" tabindex="-1">3.1 语义分块 (Semantic Chunking) <a class="header-anchor" href="#_3-1-语义分块-semantic-chunking" aria-label="Permalink to &quot;3.1 语义分块 (Semantic Chunking)&quot;">​</a></h3><ul><li><strong>核心思想</strong>：这种方法不看字数、不看标点，而是看“意思”。它会计算相邻句子或小段文字的 Embedding 向量，看看它们在语义上有多接近。当发现前后两部分的“话题”跳跃比较大（语义相似度低于某个设定的“阈值”）时，就在这个“语义断裂点”进行切割。</li><li><strong>优点</strong> ：切分点更“懂”语义，总能在话题自然转变的地方下手。这样能保证每个 Chunk 内部意思高度相关、不跑题，理论上切出来的块更符合人的阅读理解习惯。</li><li><strong>缺点</strong> ：要算 Embedding，计算开销比前面那些简单方法大得多。效果好坏非常依赖 Embedding 模型本身的能力，而且那个“语义距离阈值”得靠实验慢慢调，有点麻烦。处理速度也比较慢。</li><li><strong>适用场景</strong>：对分块质量要求很高，并且计算资源比较充裕的场景。特别适合处理那些没什么结构化标记（比如纯文本、对话记录），但意思很丰富的长文。</li></ul><blockquote><p><strong>实现思路举例</strong>： 一种可能的做法（去年我记得Langchain是这样）是，不比较单个句子，而是把连续的几个句子（比如 3 句）看成一个“语义单元”，计算这个单元的 Embedding，然后比较相邻“单元”之间的语义距离。 当然，这种方法具体效果如何，和其他语义分割的思路比起来怎么样，还需要更多的实践和对比验证,因为我没有具体测试过，后续会出一篇文章关于它的，敬请期待！</p></blockquote><h3 id="_3-2-分层分块-hierarchical-chunking" tabindex="-1">3.2 分层分块 (Hierarchical Chunking) <a class="header-anchor" href="#_3-2-分层分块-hierarchical-chunking" aria-label="Permalink to &quot;3.2 分层分块 (Hierarchical Chunking)&quot;">​</a></h3><ul><li><strong>核心思想</strong>：类似于混合策略，但更系统化地创建多个<strong>层级</strong>的 Chunks。例如，可以先将文档按章节分割成大块（L1 Chunks），再将每个章节按段落分割成中块（L2 Chunks），最后可能按句子分割成小块（L3 Chunks）。这些不同层级的 Chunks 可以被索引，并用于不同的检索策略（见下文 Small-to-Big）。</li><li><strong>优点</strong> ：提供了不同粒度的上下文信息，增加了检索的灵活性。</li><li><strong>缺点</strong> ：增加了索引的复杂度和存储空间。需要设计好多层级之间的关系和使用方式。</li><li><strong>适用场景</strong>：需要处理具有清晰层级结构的复杂文档（如书籍、长篇报告），并且希望在检索时能灵活选择上下文粒度。</li></ul><h3 id="_3-3-small-to-big-检索-父文档检索器-parent-document-retriever" tabindex="-1">3.3 Small-to-Big 检索 / 父文档检索器 (Parent Document Retriever) <a class="header-anchor" href="#_3-3-small-to-big-检索-父文档检索器-parent-document-retriever" aria-label="Permalink to &quot;3.3 Small-to-Big 检索 / 父文档检索器 (Parent Document Retriever)&quot;">​</a></h3><ul><li><strong>核心思想</strong>：这严格来说是一种<strong>检索策略</strong>，但它依赖于特定的分块方式（通常是分层或存在父子关系的块）。基本流程是：</li></ul><ol><li>将文档分割成<strong>小块</strong> (Small Chunks)，这些小块适合进行精确的向量相似度检索。</li><li>同时，保留或链接到这些小块所属的<strong>更大的父块</strong> (Parent Chunks)（例如，小块是句子，父块是包含该句子的段落）。</li><li>检索时，先用查询向量匹配<strong>小块</strong>。</li><li>一旦找到相关的小块，<strong>不直接将小块</strong>传递给 LLM，而是<strong>返回其对应的父块</strong>。</li></ol><ul><li><strong>优点</strong> ：结合了小块的<strong>检索精度</strong>和大块的<strong>上下文完整性</strong>。既能精准定位相关信息点，又能为 LLM 提供更丰富的背景。</li><li><strong>缺点</strong> ：需要维护小块和父块之间的映射关系，增加了索引和检索逻辑的复杂度。</li><li><strong>适用场景</strong>：既需要高精度检索，又需要充分上下文来生成高质量答案的 RAG 应用。</li></ul><h3 id="_3-4-命题分块-proposition-chunking" tabindex="-1">3.4 命题分块 (Proposition Chunking) <a class="header-anchor" href="#_3-4-命题分块-proposition-chunking" aria-label="Permalink to &quot;3.4 命题分块 (Proposition Chunking)&quot;">​</a></h3><ul><li><p><strong>核心思想</strong>：尝试将文本分解为更小的、原子性的<strong>事实陈述或主张（Propositions）</strong>。这通常需要借助 LLM 或专门的 NLP 模型来识别和提取文本中的核心命题。例如，句子“苹果公司在 2023 年发布了 Vision Pro 头显，定价 3499 美元”可能会被分解为：“苹果公司发布了 Vision Pro 头显”、“Vision Pro 头显发布于 2023 年”、“Vision Pro 头显定价 3499 美元”等命题。然后对这些命题进行索引和检索。</p></li><li><p><strong>优点</strong> ：产生了非常细粒度、高度聚焦的知识单元，非常适合进行精确的事实检索和问答。</p></li><li><p><strong>缺点</strong> ：</p></li><li><p>严重依赖 LLM 或 NLP 模型的抽取能力，可能产生错误或不完整的命题。</p></li><li><p>计算成本高昂，处理速度慢。</p></li><li><p>可能丢失原始文本的语气、复杂关系和细微差别。</p></li><li><p>如何将检索到的离散命题有效地组合起来提供给生成模型是一个挑战。</p></li><li><p><strong>适用场景</strong>：知识库构建、事实性问答系统，对信息的原子性和精确性要求极高的场景。</p></li></ul><h3 id="_3-5-agentic-llm-based-chunking" tabindex="-1">3.5 Agentic / LLM-based Chunking <a class="header-anchor" href="#_3-5-agentic-llm-based-chunking" aria-label="Permalink to &quot;3.5 Agentic / LLM-based Chunking&quot;">​</a></h3><ul><li><p><strong>核心思想</strong>：更进一步，让一个<strong>智能体 (Agent)</strong> 或直接使用 <strong>LLM</strong> 来<strong>决策</strong>如何进行分块。可以给 LLM 设计特定的 Prompt，让它根据内容理解来判断最佳的分割点，或者让 Agent 动态地选择和组合不同的分块策略。</p></li><li><p><strong>优点</strong> ：潜力巨大，理论上可以实现最智能、最符合语义和上下文的分块。</p></li><li><p><strong>缺点</strong> ：</p></li><li><p>实现非常复杂，需要精巧的 Prompt Engineering 或 Agent 逻辑设计。</p></li><li><p>成本高（LLM 调用开销），速度慢。</p></li><li><p>结果的可控性和稳定性可能不如确定性算法。</p></li><li><p>仍处于探索阶段，成熟度和可靠性有待验证。</p></li><li><p><strong>适用场景</strong>：研究探索性质的项目，或者对分块质量有极致追求且不计成本的特定应用。</p></li></ul><h2 id="四、块优化策略" tabindex="-1">四、块优化策略 <a class="header-anchor" href="#四、块优化策略" aria-label="Permalink to &quot;四、块优化策略&quot;">​</a></h2><p>✨ 上下文富化 (Context Enrichment) - (补充策略)</p><ul><li><strong>核心思想</strong>：这不是一种独立的分割方法，而是在分块<strong>之后</strong>，为每个 Chunk <strong>添加额外的上下文信息</strong>。例如，可以在每个 Chunk 前后添加其相邻的句子或摘要信息，或者添加该 Chunk 所属章节/段落的标题作为元数据（如 MarkdownHeaderTextSplitter 所做）。</li><li><strong>优点</strong> ：在不显著增大 Chunk 大小的情况下，为 LLM 提供更多线索，帮助其理解 Chunk 在原文中的位置和背景。</li><li><strong>缺点</strong> ：需要额外的处理步骤来提取和添加这些富化信息。</li><li><strong>适用场景</strong>：可以与其他任何分块策略结合使用，作为一种优化手段，特别是在 Chunk 较小，担心上下文不足时。</li></ul><p><strong>小结</strong>：高级策略提供了更精细化、更智能化的分块选择，但往往伴随着更高的复杂度和计算成本。选择哪种策略，需要在具体应用场景下，仔细权衡效果、成本和实现复杂度。</p></div></div></main><footer class="VPDocFooter" data-v-06ef2308 data-v-c461bbcf><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-c461bbcf><span class="visually-hidden" id="doc-footer-aria-label" data-v-c461bbcf>Pager</span><div class="pager" data-v-c461bbcf><a class="VPLink link pager-link prev" href="/ai/rag/rag3.html" data-v-c461bbcf><!--[--><span class="desc" data-v-c461bbcf>Previous page</span><span class="title" data-v-c461bbcf>快速搭建支持多模态文档的 RAG 问答系统</span><!--]--></a></div><div class="pager" data-v-c461bbcf><!----></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-32e39dcf data-v-5458bf99><div class="container" data-v-5458bf99><p class="message" data-v-5458bf99>用心记录，温暖分享</p><p class="copyright" data-v-5458bf99> 2024 Daci Blog</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_fitness_index.md\":\"Bou26D0Q\",\"about_fitness_training_warmup1.md\":\"DaJAcpIR\",\"about_fitness_training_warmup2.md\":\"CW0x0uvD\",\"about_life_index.md\":\"CXbzT0g_\",\"about_life_thoughts_growth-mindset.md\":\"DothoJZu\",\"ai_cursor.md\":\"DoIxipQl\",\"ai_index.md\":\"BXdtWxVV\",\"ai_rag_rag1.md\":\"DcqMX1H9\",\"ai_rag_rag2.md\":\"u6NiAyZc\",\"ai_rag_rag3.md\":\"CQNjp5Y9\",\"ai_rag_rag4.md\":\"D9UiMew5\",\"ai_windsurf.md\":\"rI8GWiq6\",\"en_about_fitness_index.md\":\"CR4hSf6R\",\"en_about_fitness_training_warmup1.md\":\"CL_G46HX\",\"en_about_fitness_training_warmup2.md\":\"BH_bcG88\",\"en_about_life_index.md\":\"chI4W3Wr\",\"en_about_life_thoughts_growth-mindset.md\":\"ByVm340J\",\"en_ai_cursor.md\":\"BKBwgDf3\",\"en_ai_index.md\":\"C_fO7ew3\",\"en_ai_rag_rag1.md\":\"BgQfIkUI\",\"en_ai_rag_rag2.md\":\"BKIpR57u\",\"en_ai_rag_rag3.md\":\"WiNxM9uH\",\"en_ai_rag_rag4.md\":\"OUTPwYaE\",\"en_ai_windsurf.md\":\"C_QNWWj4\",\"en_index.md\":\"CMKipQKC\",\"en_markdown-examples.md\":\"Bur12mIb\",\"en_tech_backend_index.md\":\"ltmUyQ_J\",\"en_tech_frontend_20250307.md\":\"DlVZMUfG\",\"en_tech_frontend_index.md\":\"DIJIjZrz\",\"en_tech_ops_docker_basic-usage.md\":\"yV3omPMd\",\"en_tech_ops_index.md\":\"CXfV4Buh\",\"en_tools_index.md\":\"BizG2mWx\",\"index.md\":\"BZUE4Mp3\",\"tech_backend_index.md\":\"BzUtPg_W\",\"tech_frontend_20250307.md\":\"ZBu71huP\",\"tech_frontend_index.md\":\"C4U-7EDh\",\"tech_frontend_javascript-basics.md\":\"CQqAw1sd\",\"tech_ops_docker_basic-usage.md\":\"DkYSGKfn\",\"tech_ops_index.md\":\"Ca3w2Wmr\",\"tools_index.md\":\"Cx0LaOrB\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"Daci Blog\",\"description\":\"Daci的博客\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"siteTitle\":\"Daci\",\"logo\":\"/logo.svg\",\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/zhangdanqing/zhangdanqing.github.io\"}],\"footer\":{\"message\":\"用心记录，温暖分享\",\"copyright\":\" 2024 Daci Blog\"}},\"locales\":{\"root\":{\"label\":\"简体中文\",\"lang\":\"zh-CN\",\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"AI\",\"link\":\"/ai/\"},{\"text\":\"技术\",\"items\":[{\"text\":\"前端\",\"link\":\"/tech/frontend/\"},{\"text\":\"后端\",\"link\":\"/tech/backend/\"}]},{\"text\":\"工具\",\"link\":\"/tools/\"},{\"text\":\"关于\",\"items\":[{\"text\":\"健身\",\"link\":\"/about/fitness/\"},{\"text\":\"生活\",\"link\":\"/about/life/\"}]}],\"sidebar\":{\"/ai/\":[{\"text\":\"AI\",\"items\":[{\"text\":\"AI 导览\",\"link\":\"/ai/\"},{\"text\":\"Cursor中AI的应用\",\"link\":\"/ai/cursor\"},{\"text\":\"深度探索Windsurf编辑器中AI的应用\",\"link\":\"/ai/windsurf\"},{\"text\":\"解析RAG\",\"link\":\"/ai/rag/rag1\"},{\"text\":\"大模型应用框架解析：RAG、Agent、微调、提示词工程\",\"link\":\"/ai/rag/rag2\"},{\"text\":\"快速搭建支持多模态文档的 RAG 问答系统\",\"link\":\"/ai/rag/rag3\"},{\"text\":\"RAG 文本分块，从策略到优化\",\"link\":\"/ai/rag/rag4\"}]}],\"/tools/\":[{\"text\":\"工具\",\"items\":[{\"text\":\"工具导览\",\"link\":\"/tools/\"}]}],\"/tech/frontend/\":[{\"text\":\"前端开发\",\"collapsed\":false,\"items\":[{\"text\":\"Next.js全栈实战：手把手集成NextAuth实现Google/GitHub一键登录\",\"link\":\"/tech/frontend/20250307\"},{\"text\":\"NocoBase 用户指南\",\"link\":\"/tech/frontend/\"}]}],\"/tech/backend/\":[{\"text\":\"后端开发\",\"collapsed\":true,\"items\":[{\"text\":\"使用 NestJS 搭建后端项目\",\"link\":\"/tech/backend/\"}]}],\"/about/fitness/\":[{\"text\":\"健身\",\"items\":[{\"text\":\"背部训练-上背和肩后束\",\"link\":\"/about/fitness/training/warmUp2\"},{\"text\":\"上肢热身/纠正性训练\",\"link\":\"/about/fitness/training/warmUp1\"},{\"text\":\"肩袖肌群受伤、肩关节弹响\",\"link\":\"/about/fitness/\"}]}],\"/about/life/\":[{\"text\":\"生活\",\"items\":[{\"text\":\"关于生活与成长的思考\",\"link\":\"/about/life/\"}]}]}}},\"en\":{\"label\":\"English\",\"lang\":\"en-US\",\"themeConfig\":{\"nav\":[{\"text\":\"Home\",\"link\":\"/en/\"},{\"text\":\"AI\",\"link\":\"/en/ai/\"},{\"text\":\"Tech\",\"items\":[{\"text\":\"Frontend\",\"link\":\"/en/tech/frontend/\"},{\"text\":\"Backend\",\"link\":\"/en/tech/backend/\"}]},{\"text\":\"Tools\",\"link\":\"/en/tools/\"},{\"text\":\"About\",\"items\":[{\"text\":\"Fitness\",\"link\":\"/en/about/fitness/\"},{\"text\":\"Life\",\"link\":\"/en/about/life/\"}]}],\"sidebar\":{\"/en/ai/\":[{\"text\":\"AI\",\"items\":[{\"text\":\"AI Tools Guide\",\"link\":\"/en/ai/\"},{\"text\":\"AI Applications in Cursor\",\"link\":\"/en/ai/cursor\"},{\"text\":\"Exploring AI in Windsurf Editor\",\"link\":\"/en/ai/windsurf\"},{\"text\":\"RAG Analysis\",\"link\":\"en/ai/rag/rag1\"},{\"text\":\"Analysis of Large Model Application Framework: RAG, Agent, Fine tuning, Prompt Word Engineering\",\"link\":\"en/ai/rag/rag2\"},{\"text\":\"Quickly build a RAG Q&A system that supports multimodal documents\",\"link\":\"en/ai/rag/rag3\"},{\"text\":\"RAG text segmentation, from strategy to optimization\",\"link\":\"en/ai/rag/rag4\"}]}],\"/en/tools/\":[{\"text\":\"Tools\",\"items\":[{\"text\":\"Tools Guide\",\"link\":\"/en/tools/\"}]}],\"/en/tech/frontend/\":[{\"text\":\"Frontend Development\",\"collapsed\":false,\"items\":[{\"text\":\"Next.js Full-Stack Practice: Step-by-Step Integration of NextAuth for Google/GitHub One-Click Login\",\"link\":\"/en/tech/frontend/20250307\"},{\"text\":\"NocoBase User Guide\",\"link\":\"/en/tech/frontend/\"}]}],\"/en/tech/backend/\":[{\"text\":\"Backend Development\",\"collapsed\":true,\"items\":[{\"text\":\"Building a Backend Project with NestJS\",\"link\":\"/en/tech/backend/\"}]}],\"/en/about/fitness/\":[{\"text\":\"Fitness\",\"items\":[{\"text\":\"Back Training - Upper Back and Rear Deltoid\",\"link\":\"/en/about/fitness/training/warmUp2\"},{\"text\":\"Upper limb warm-up/corrective training\",\"link\":\"/en/about/fitness/training/warmUp1\"},{\"text\":\"Shoulder muscle group injury, shoulder joint popping\",\"link\":\"/en/about/fitness/\"}]}],\"/en/about/life/\":[{\"text\":\"Life\",\"items\":[{\"text\":\"Thoughts on Personal Work and Life\",\"link\":\"/en/about/life/\"}]}]}}}},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>