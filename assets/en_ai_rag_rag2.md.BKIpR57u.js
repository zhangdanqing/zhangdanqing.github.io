import{_ as t,c as n,o as a,ae as i}from"./chunks/framework.BjlC_BXf.js";const u=JSON.parse('{"title":"Below is a structured summary of the core relationships and differences among RAG (Retrieval-Augmented Generation), Agent (Intelligent Agent), and Fine-tuning, using comparative tables and scenario-based explanations:","description":"","frontmatter":{},"headers":[],"relativePath":"en/ai/rag/rag2.md","filePath":"en/ai/rag/rag2.md"}'),o={name:"en/ai/rag/rag2.md"};function r(s,e,l,g,d,c){return a(),n("div",null,e[0]||(e[0]=[i(`<h2 id="i-rag-retrieval-augmented-generation" tabindex="-1">I. <strong>RAG (Retrieval-Augmented Generation)</strong> <a class="header-anchor" href="#i-rag-retrieval-augmented-generation" aria-label="Permalink to &quot;I. **RAG (Retrieval-Augmented Generation)**&quot;">​</a></h2><p>RAG (Retrieval-Augmented Generation) is a generation technology enhanced by retrieval. Its core idea is to retrieve task-related knowledge from external knowledge bases <strong>before generating text</strong> and feed this information as part of the input to improve the accuracy and relevance of generated text.</p><h4 id="_1-technical-features" tabindex="-1"><strong>1. Technical Features</strong> <a class="header-anchor" href="#_1-technical-features" aria-label="Permalink to &quot;**1. Technical Features**&quot;">​</a></h4><ul><li><strong>Low cost of knowledge updating:</strong> There is no need to retrain the entire model; simply updating the knowledge base can realize knowledge updates and expansion.</li><li><strong>Improved answer accuracy:</strong> By retrieving relevant knowledge, RAG can provide more accurate and relevant answers.</li><li><strong>Enhanced interpretability:</strong> Since generated text is based on retrievable knowledge, users can verify the accuracy of answers, increasing trust in model outputs.</li></ul><h4 id="_2-application-scenarios" tabindex="-1"><strong>2. Application Scenarios</strong> <a class="header-anchor" href="#_2-application-scenarios" aria-label="Permalink to &quot;**2. Application Scenarios**&quot;">​</a></h4><p>RAG is suitable for scenarios requiring a large amount of external knowledge, such as knowledge-intensive tasks, AI document Q&amp;A, business training, and scientific research.</p><h4 id="_3-technical-highlights" tabindex="-1"><strong>3. Technical Highlights</strong> <a class="header-anchor" href="#_3-technical-highlights" aria-label="Permalink to &quot;**3. Technical Highlights**&quot;">​</a></h4><ul><li><strong>Retrieval phase:</strong> Uses encoding models (such as BM25, SentenceBERT, ColBERT, etc.) to retrieve relevant information from the knowledge base.</li><li><strong>Generation phase:</strong> Generates text based on the retrieved information and the specific requirements of the task.</li></ul><h4 id="_4-limitations" tabindex="-1"><strong>4. Limitations</strong> <a class="header-anchor" href="#_4-limitations" aria-label="Permalink to &quot;**4. Limitations**&quot;">​</a></h4><ul><li><strong>Dependent on external knowledge base:</strong> Performance is affected by the quality and scale of the knowledge base.</li><li><strong>Retrieval module challenges:</strong> If no relevant information is retrieved or the retrieved information is inaccurate, the quality of generated text will be affected.</li></ul><h4 id="_5-outlook" tabindex="-1"><strong>5. Outlook</strong> <a class="header-anchor" href="#_5-outlook" aria-label="Permalink to &quot;**5. Outlook**&quot;">​</a></h4><p>With technological progress, RAG will further improve in retrieval and generation performance and expand to more application scenarios, such as enterprise knowledge base construction and intelligent customer service.</p><h4 id="_6-related-products" tabindex="-1"><strong>6. Related Products</strong> <a class="header-anchor" href="#_6-related-products" aria-label="Permalink to &quot;**6. Related Products**&quot;">​</a></h4><ul><li><strong>RAGFlow:</strong> Aims to build a search-centric, end-to-end optimized system to address the limitations of RAG 1.0.</li><li><strong>GraphRAG:</strong> An open-source graph-based RAG method by Microsoft. It uses LLMs to extract knowledge graphs from data sources, then clusters the graphs into entity communities at different granularities. For RAG operations, all communities are traversed to create &quot;community answers,&quot; which are then reduced to produce final answers.</li></ul><hr><h2 id="ii-agent-intelligent-agent" tabindex="-1">II. <strong>Agent (Intelligent Agent)</strong> <a class="header-anchor" href="#ii-agent-intelligent-agent" aria-label="Permalink to &quot;II. **Agent (Intelligent Agent)**&quot;">​</a></h2><p>Agents empower software entities with autonomy and interactivity, enabling them to respond intelligently and flexibly to environmental changes and user needs. &quot;Agent&quot; is often translated as &quot;proxy&quot; or &quot;intelligent agent&quot; and is a foundation for constructing intelligent systems. In Agent systems, the LLM itself acts as the &quot;brain&quot; of the agent, performing multi-step reasoning according to user-specified tasks, providing task execution steps and methods, and ultimately accomplishing tasks by invoking external APIs or methods.</p><h4 id="_1-technical-features-1" tabindex="-1"><strong>1. Technical Features</strong> <a class="header-anchor" href="#_1-technical-features-1" aria-label="Permalink to &quot;**1. Technical Features**&quot;">​</a></h4><ul><li><strong>Autonomy:</strong> Thinks, plans, and decides actions based on internal state and environmental information.</li><li><strong>Reactivity:</strong> Perceives and responds to information from the environment.</li><li><strong>Sociality:</strong> Interacts with other agents via communication protocols.</li><li><strong>Proactivity:</strong> Actively initiates actions to pursue its goals or meet user needs.</li></ul><h4 id="_2-application-scenarios-1" tabindex="-1"><strong>2. Application Scenarios</strong> <a class="header-anchor" href="#_2-application-scenarios-1" aria-label="Permalink to &quot;**2. Application Scenarios**&quot;">​</a></h4><p>Agent technology is widely used in e-commerce (intelligent recommendation, personalized services), intelligent manufacturing (autonomous device control, collaborative production), intelligent transportation (vehicle navigation, traffic monitoring), smart homes, and more.</p><h4 id="_3-technical-highlights-1" tabindex="-1"><strong>3. Technical Highlights</strong> <a class="header-anchor" href="#_3-technical-highlights-1" aria-label="Permalink to &quot;**3. Technical Highlights**&quot;">​</a></h4><ul><li><strong>Autonomous decision-making:</strong> Makes decisions after multi-round reasoning based on internal algorithms and external environmental information.</li><li><strong>Efficient communication:</strong> Interacts with other agents or users via communication protocols.</li></ul><h4 id="_4-limitations-1" tabindex="-1"><strong>4. Limitations</strong> <a class="header-anchor" href="#_4-limitations-1" aria-label="Permalink to &quot;**4. Limitations**&quot;">​</a></h4><ul><li><strong>Security concerns:</strong> As application scope expands, data security and privacy protection become especially important.</li><li><strong>Lack of standardization:</strong> Interoperability between different agents needs improvement.</li></ul><h4 id="_5-outlook-1" tabindex="-1"><strong>5. Outlook</strong> <a class="header-anchor" href="#_5-outlook-1" aria-label="Permalink to &quot;**5. Outlook**&quot;">​</a></h4><p>With technological development, agents will become more intelligent and collaborative, playing roles in more domains while emphasizing security and standardization.</p><h4 id="_6-related-products-1" tabindex="-1"><strong>6. Related Products</strong> <a class="header-anchor" href="#_6-related-products-1" aria-label="Permalink to &quot;**6. Related Products**&quot;">​</a></h4><p>Currently, there are few mature general-purpose products in the Agent domain; development is generally done using LLM frameworks such as LangChain.</p><hr><h2 id="iii-prompt-engineering" tabindex="-1">III. <strong>Prompt Engineering</strong> <a class="header-anchor" href="#iii-prompt-engineering" aria-label="Permalink to &quot;III. **Prompt Engineering**&quot;">​</a></h2><p>Prompt engineering is a technique used in artificial intelligence, especially with large language models (LLMs), to guide the model in generating specific outputs by providing clear and concrete instructions. In daily use of LLM tools for Q&amp;A, document generation, and more, prompt engineering is employed.</p><h4 id="_1-features" tabindex="-1"><strong>1. Features</strong> <a class="header-anchor" href="#_1-features" aria-label="Permalink to &quot;**1. Features**&quot;">​</a></h4><ul><li><strong>Low threshold:</strong> Interact with the model using natural language.</li><li><strong>High controllability:</strong> Can more accurately describe tasks.</li><li><strong>Low cost:</strong> Does not require extra datasets or computational resources.</li></ul><h4 id="_2-application-scenarios-2" tabindex="-1"><strong>2. Application Scenarios</strong> <a class="header-anchor" href="#_2-application-scenarios-2" aria-label="Permalink to &quot;**2. Application Scenarios**&quot;">​</a></h4><p>Prompt engineering is widely used in text generation, Q&amp;A systems, dialogue systems, and other domains.</p><h4 id="_3-technical-highlights-2" tabindex="-1"><strong>3. Technical Highlights</strong> <a class="header-anchor" href="#_3-technical-highlights-2" aria-label="Permalink to &quot;**3. Technical Highlights**&quot;">​</a></h4><ul><li><strong>Structured prompts:</strong> Built using a combination of templates, examples, and user input.</li><li><strong>Prompt optimization:</strong> Improves the quality and accuracy of generated outputs.</li></ul><h4 id="_4-limitations-2" tabindex="-1"><strong>4. Limitations</strong> <a class="header-anchor" href="#_4-limitations-2" aria-label="Permalink to &quot;**4. Limitations**&quot;">​</a></h4><ul><li><strong>Model dependence:</strong> The effect of prompts depends on model performance.</li><li><strong>Complexity:</strong> Designing effective prompts for complex tasks can be challenging.</li></ul><h4 id="_5-outlook-2" tabindex="-1"><strong>5. Outlook</strong> <a class="header-anchor" href="#_5-outlook-2" aria-label="Permalink to &quot;**5. Outlook**&quot;">​</a></h4><p>As LLMs advance, prompt engineering will become more intelligent and automated, further reducing user interaction costs.</p><h4 id="_6-related-products-2" tabindex="-1"><strong>6. Related Products</strong> <a class="header-anchor" href="#_6-related-products-2" aria-label="Permalink to &quot;**6. Related Products**&quot;">​</a></h4><p><strong>PromptPerfect:</strong> A prompt optimization tool designed for large language models, aiming to enhance the performance of large language and foundation models by simplifying and optimizing prompt engineering.</p><hr><h2 id="iv-fine-tuning" tabindex="-1">IV. <strong>Fine-tuning</strong> <a class="header-anchor" href="#iv-fine-tuning" aria-label="Permalink to &quot;IV. **Fine-tuning**&quot;">​</a></h2><p>Fine-tuning is the process of further training a pre-trained LLM on a new, task-specific dataset. The main goal of fine-tuning is to adapt the model for a new, specific task or domain without having to train a brand-new model from scratch.</p><p>Through fine-tuning, the general model&#39;s understanding and generation ability for a specific domain or industry can be enhanced, achieving good results in industry applications of LLMs.</p><h4 id="_1-technical-features-2" tabindex="-1"><strong>1. Technical Features</strong> <a class="header-anchor" href="#_1-technical-features-2" aria-label="Permalink to &quot;**1. Technical Features**&quot;">​</a></h4><ul><li><strong>Strengthens model&#39;s existing knowledge:</strong> Makes full use of the general features of pre-trained models.</li><li><strong>Suitable for complex instructions:</strong> Improves model interaction efficiency.</li></ul><h4 id="_2-application-scenarios-3" tabindex="-1"><strong>2. Application Scenarios</strong> <a class="header-anchor" href="#_2-application-scenarios-3" aria-label="Permalink to &quot;**2. Application Scenarios**&quot;">​</a></h4><p>Fine-tuning is widely applied in computer vision (image classification, object detection), natural language processing (text classification, sentiment analysis, machine translation), and more.</p><h4 id="_3-technical-highlights-3" tabindex="-1"><strong>3. Technical Highlights</strong> <a class="header-anchor" href="#_3-technical-highlights-3" aria-label="Permalink to &quot;**3. Technical Highlights**&quot;">​</a></h4><ul><li><strong>Pre-trained models:</strong> Use large amounts of unlabeled or weakly labeled data for pre-training.</li><li><strong>Task-specific fine-tuning:</strong> Use labeled datasets to fine-tune the pre-trained model.</li></ul><h4 id="_4-limitations-3" tabindex="-1"><strong>4. Limitations</strong> <a class="header-anchor" href="#_4-limitations-3" aria-label="Permalink to &quot;**4. Limitations**&quot;">​</a></h4><ul><li><strong>High computational resource consumption:</strong> All trainable weight parameters need to be opened.</li><li><strong>Long training time:</strong> Updating the entire model&#39;s parameters takes relatively long.</li><li><strong>Prone to overfitting:</strong> Overfitting can occur if resources are limited or data is insufficient.</li></ul><h4 id="_5-outlook-3" tabindex="-1"><strong>5. Outlook</strong> <a class="header-anchor" href="#_5-outlook-3" aria-label="Permalink to &quot;**5. Outlook**&quot;">​</a></h4><p>With increased computational resources and algorithmic optimization, fine-tuning will become more efficient and accurate, adapting to more complex tasks.</p><h4 id="_6-related-products-3" tabindex="-1"><strong>6. Related Products</strong> <a class="header-anchor" href="#_6-related-products-3" aria-label="Permalink to &quot;**6. Related Products**&quot;">​</a></h4><ul><li><strong>Hugging Face:</strong> Provides various pre-trained models and fine-tuning tools, supporting multiple languages and tasks.</li><li><strong>Fine-Tuner.ai:</strong> Creates professional AI models based on user data.</li><li><strong>LLaMa-factory:</strong> An open-source, low-code large model training framework integrating the industry&#39;s most widely used fine-tuning methods and optimization techniques. It supports fine-tuning and secondary training of many open-source models, and provides a visual training and inference platform for zero-code fine-tuning.</li></ul><hr><h1 id="below-is-a-structured-summary-of-the-core-relationships-and-differences-among-rag-retrieval-augmented-generation-agent-intelligent-agent-and-fine-tuning-using-comparative-tables-and-scenario-based-explanations" tabindex="-1">Below is a structured summary of the core relationships and differences among <strong>RAG (Retrieval-Augmented Generation)</strong>, <strong>Agent (Intelligent Agent)</strong>, and <strong>Fine-tuning</strong>, using comparative tables and scenario-based explanations: <a class="header-anchor" href="#below-is-a-structured-summary-of-the-core-relationships-and-differences-among-rag-retrieval-augmented-generation-agent-intelligent-agent-and-fine-tuning-using-comparative-tables-and-scenario-based-explanations" aria-label="Permalink to &quot;Below is a structured summary of the core relationships and differences among **RAG (Retrieval-Augmented Generation)**, **Agent (Intelligent Agent)**, and **Fine-tuning**, using comparative tables and scenario-based explanations:&quot;">​</a></h1><hr><h3 id="_1-core-definitions-and-objectives" tabindex="-1"><strong>1. Core Definitions and Objectives</strong> <a class="header-anchor" href="#_1-core-definitions-and-objectives" aria-label="Permalink to &quot;**1. Core Definitions and Objectives**&quot;">​</a></h3><table tabindex="0"><thead><tr><th style="text-align:center;"><strong>Technology</strong></th><th style="text-align:center;"><strong>Core Objective</strong></th><th style="text-align:center;"><strong>Key Capability</strong></th></tr></thead><tbody><tr><td style="text-align:center;"><strong>RAG</strong></td><td style="text-align:center;">Enhance <strong>factual accuracy</strong> and <strong>timeliness</strong> of generated content</td><td style="text-align:center;">Retrieve relevant info from external KB, assist generation</td></tr><tr><td style="text-align:center;"><strong>Agent</strong></td><td style="text-align:center;">Complete <strong>multi-step complex tasks</strong></td><td style="text-align:center;">Planning, tool invocation, memory, iterative decisions</td></tr><tr><td style="text-align:center;"><strong>Fine-tuning</strong></td><td style="text-align:center;">Enable LLM adaptation to <strong>specific domains or styles</strong></td><td style="text-align:center;">Adjust model parameters via training</td></tr></tbody></table><hr><h3 id="_2-core-differences" tabindex="-1"><strong>2. Core Differences</strong> <a class="header-anchor" href="#_2-core-differences" aria-label="Permalink to &quot;**2. Core Differences**&quot;">​</a></h3><table tabindex="0"><thead><tr><th style="text-align:center;"><strong>Dimension</strong></th><th style="text-align:center;"><strong>RAG</strong></th><th style="text-align:center;"><strong>Agent</strong></th><th style="text-align:center;"><strong>Fine-tuning</strong></th></tr></thead><tbody><tr><td style="text-align:center;"><strong>Nature</strong></td><td style="text-align:center;">Knowledge retrieval + generation enhancement</td><td style="text-align:center;">Task execution system</td><td style="text-align:center;">Model parameter optimization</td></tr><tr><td style="text-align:center;"><strong>Data Dependency</strong></td><td style="text-align:center;">Requires vector DB / document store</td><td style="text-align:center;">Requires tool APIs + environment interfaces</td><td style="text-align:center;">Requires labeled training data</td></tr><tr><td style="text-align:center;"><strong>Update Method</strong></td><td style="text-align:center;">Dynamically update knowledge base</td><td style="text-align:center;">Adjust planning logic or toolset</td><td style="text-align:center;">Retrain the model</td></tr><tr><td style="text-align:center;"><strong>Application Scenarios</strong></td><td style="text-align:center;">Knowledge-intensive QA, real-time info generation</td><td style="text-align:center;">Automated workflow, multi-tool collaboration</td><td style="text-align:center;">Domain-specialized output (e.g. legal, medical)</td></tr><tr><td style="text-align:center;"><strong>Cost</strong></td><td style="text-align:center;">Low (only maintain knowledge base)</td><td style="text-align:center;">Medium (toolchain development)</td><td style="text-align:center;">High (training hardware needed)</td></tr></tbody></table><hr><h3 id="_3-synergistic-relationships" tabindex="-1"><strong>3. Synergistic Relationships</strong> <a class="header-anchor" href="#_3-synergistic-relationships" aria-label="Permalink to &quot;**3. Synergistic Relationships**&quot;">​</a></h3><h4 id="typical-collaboration-pattern" tabindex="-1"><strong>Typical Collaboration Pattern</strong> <a class="header-anchor" href="#typical-collaboration-pattern" aria-label="Permalink to &quot;**Typical Collaboration Pattern**&quot;">​</a></h4><div class="language-mermaid vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mermaid</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">graph TB</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    A[User Input] --&gt; B(Agent Task Decomposition)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    B --&gt; C{Need domain knowledge?}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    C --&gt;|Yes| D[RAG Retrieval-Augmentation]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    C --&gt;|No| E[Tool Invocation]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    D --&gt; F[Fine-tuned LLM Generation]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    E --&gt; F</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    F --&gt; G[Result Verification]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    G --&gt; H[Output or Iteration]</span></span></code></pre></div><h4 id="collaboration-explanation" tabindex="-1"><strong>Collaboration Explanation</strong> <a class="header-anchor" href="#collaboration-explanation" aria-label="Permalink to &quot;**Collaboration Explanation**&quot;">​</a></h4><ol><li><strong>Agent as the Central Coordinator</strong><ul><li>Decides when to invoke RAG (for knowledge retrieval) or tools (for action execution)</li><li>Example: A customer service agent first uses RAG to query policy, then calls an order API to modify data</li></ul></li><li><strong>RAG Provides Real-Time Knowledge</strong><ul><li>Supplements the fine-tuned model with the latest information (avoiding outdated knowledge)</li><li>Example: A medical agent uses RAG to retrieve the latest papers, then generates diagnostic advice with a fine-tuned model</li></ul></li><li><strong>Fine-tuning Optimizes Core Abilities</strong><ul><li>Improves understanding of domain terminology (e.g. legal clause parsing)</li><li>Adapts generation style (e.g. simplified language for children&#39;s education)</li></ul></li></ol><hr><h3 id="_4-scenario-based-comparison" tabindex="-1"><strong>4. Scenario-Based Comparison</strong> <a class="header-anchor" href="#_4-scenario-based-comparison" aria-label="Permalink to &quot;**4. Scenario-Based Comparison**&quot;">​</a></h3><h4 id="example-intelligent-medical-assistant" tabindex="-1"><strong>Example: Intelligent Medical Assistant</strong> <a class="header-anchor" href="#example-intelligent-medical-assistant" aria-label="Permalink to &quot;**Example: Intelligent Medical Assistant**&quot;">​</a></h4><table tabindex="0"><thead><tr><th style="text-align:center;"><strong>Technology</strong></th><th style="text-align:center;"><strong>Role</strong></th><th style="text-align:center;"><strong>Specific Function</strong></th></tr></thead><tbody><tr><td style="text-align:center;"><strong>Fine-tuning</strong></td><td style="text-align:center;">Professional term understanding</td><td style="text-align:center;">Enables the model to master medical terminology and diagnostic logic</td></tr><tr><td style="text-align:center;"><strong>RAG</strong></td><td style="text-align:center;">Real-time knowledge supplementation</td><td style="text-align:center;">Retrieves the latest medical guidelines and case databases</td></tr><tr><td style="text-align:center;"><strong>Agent</strong></td><td style="text-align:center;">Diagnosis workflow management</td><td style="text-align:center;">Stepwise execution: consultation → examination suggestion → treatment plan generation</td></tr></tbody></table><hr><h3 id="_5-how-to-choose-a-technical-combination" tabindex="-1"><strong>5. How to Choose a Technical Combination?</strong> <a class="header-anchor" href="#_5-how-to-choose-a-technical-combination" aria-label="Permalink to &quot;**5. How to Choose a Technical Combination?**&quot;">​</a></h3><ul><li><strong>RAG first:</strong><br> For scenarios needing real-time knowledge updates but without complex task flows (e.g., FAQ Q&amp;A)</li><li><strong>Agent first:</strong><br> For processes involving multi-system interaction (e.g., booking → payment → notification)</li><li><strong>Fine-tuning first:</strong><br> For high domain expertise demands and stable knowledge (e.g., legal document generation)</li><li><strong>Combine all three:</strong><br> For complex domains + real-time knowledge + multi-tool collaboration (e.g., research assistant: retrieve papers → experiment design → data analysis)</li></ul><hr><h3 id="summary" tabindex="-1"><strong>Summary</strong> <a class="header-anchor" href="#summary" aria-label="Permalink to &quot;**Summary**&quot;">​</a></h3><ul><li><strong>RAG</strong> is the model&#39;s &quot;external memory,&quot; addressing <strong>knowledge limitations</strong></li><li><strong>Agent</strong> is the model&#39;s &quot;limbs,&quot; addressing <strong>action limitations</strong></li><li><strong>Fine-tuning</strong> is the model&#39;s &quot;brain structure transformation,&quot; addressing <strong>domain adaptation</strong></li><li><strong>Best Practice:</strong> Use Agent as the orchestrator, combine RAG for real-time knowledge, and Fine-tuning for expertise, to build a closed-loop intelligent system.</li></ul>`,84)]))}const m=t(o,[["render",r]]);export{u as __pageData,m as default};
