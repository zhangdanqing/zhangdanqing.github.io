import{_ as t,c as a,o as n,ae as i}from"./chunks/framework.BjlC_BXf.js";const g=JSON.parse('{"title":"I. RAG","description":"","frontmatter":{},"headers":[],"relativePath":"en/ai/rag/rag1.md","filePath":"en/ai/rag/rag1.md"}'),r={name:"en/ai/rag/rag1.md"};function o(s,e,l,c,d,u){return n(),a("div",null,e[0]||(e[0]=[i('<h1 id="i-rag" tabindex="-1"><strong>I. RAG</strong> <a class="header-anchor" href="#i-rag" aria-label="Permalink to &quot;**I. RAG**&quot;">​</a></h1><h2 id="_1-what-is-rag" tabindex="-1"><strong>1. What Is RAG</strong> <a class="header-anchor" href="#_1-what-is-rag" aria-label="Permalink to &quot;**1. What Is RAG**&quot;">​</a></h2><p>Retrieval-Augmented Generation (RAG) is an architecture that combines <strong>dynamic information retrieval</strong> with <strong>static knowledge generation</strong>. It works by extracting relevant fragments from structured knowledge bases in real time, providing contextual anchors for large language models (LLMs).</p><p>RAG supplements the data in a large language model with external knowledge sources of your choice, such as data repositories, collections of texts, and existing documents. This method first splits these resources into smaller chunks, indexes them in a vector database, and then uses them as references to deliver more accurate answers.</p><p>The practical value of RAG lies in its ability to direct the LLM to retrieve specific, real-time information from one or more factual sources you select. RAG can save costs by providing a customized experience without incurring the expenses of model training and fine-tuning. It also allows only the most relevant information to be sent when querying the LLM (rather than lengthy, redundant documents), thus conserving resources.</p><h2 id="_2-principles" tabindex="-1"><strong>2. Principles</strong> <a class="header-anchor" href="#_2-principles" aria-label="Permalink to &quot;**2. Principles**&quot;">​</a></h2><p>The RAG architecture operates by retrieving data from external sources, processing it into the context of the LLM, and then generating answers based on these combined sources. This process involves three main stages: data preparation, retrieval, and generation.</p><h3 id="step-1-data-preparation-for-retrieval" tabindex="-1"><strong>Step 1: Data Preparation (For Retrieval)</strong> <a class="header-anchor" href="#step-1-data-preparation-for-retrieval" aria-label="Permalink to &quot;**Step 1: Data Preparation (For Retrieval)**&quot;">​</a></h3><ul><li><p><strong>Source Document Acquisition and Loading:</strong> Identify and acquire the source documents to be shared with the LLM, ensuring they are in a format the model can interpret—typically text files, database tables, or PDFs. Regardless of the original format, each document must be converted into a text file before embedding in the vector database. This is also known as the ETL phase: Extract, Transform, and Load. ETL ensures raw data is cleaned and organized, making it ready for storage, analysis, and machine learning.</p></li><li><p><strong>Transformation:</strong> Perform &quot;text splitting&quot; or &quot;chunking&quot; to prepare documents for retrieval. This means parsing the updated documents and organizing them into relevant &quot;chunks&quot; according to various features. For example, a document organized by paragraphs may be easier for the model to search and retrieve than one arranged by tables and charts.</p><p>Chunking can be based on semantics, sentences, tokens, formatting, HTML characters, or code types. Many open-source frameworks facilitate document injection, including <a href="https://www.llamaindex.ai/open-source" target="_blank" rel="noreferrer">LlamaIndex</a> and <a href="https://www.langchain.com/" target="_blank" rel="noreferrer">LangChain</a>.</p></li><li><p><strong>Embedding:</strong> Embedding uses specialized machine learning models (vector embedding models) to convert data into numerical vectors, enabling mathematical operations to assess similarity and difference between data. Embeddings allow text (or images) to be converted into vectors that capture the core meaning of the content while discarding irrelevant details. The embedding process assigns a numerical value (e.g., [1.2, -0.9, 0.3]) to each data chunk and indexes it within a larger system such as a vector database.</p><p>In the vector database, these values help the RAG architecture indicate associations between content blocks and organize the data to optimize retrieval. This indexing approach is designed to structure vectors such that similar concepts are stored near each other in the coordinate space. For example, &quot;coffee&quot; and &quot;tea&quot; will be close together, as will &quot;hot beverage.&quot; Unrelated concepts like &quot;mobile phone&quot; and &quot;television&quot; will be farther away. The distance or proximity between two vector points helps the model decide which information to retrieve and include in the output for a user query.</p></li><li><p><strong>Storage:</strong> Combined data from multiple sources (your chosen external documents and the LLM) are stored in a central repository.</p></li></ul><h3 id="step-2-retrieval" tabindex="-1"><strong>Step 2: Retrieval</strong> <a class="header-anchor" href="#step-2-retrieval" aria-label="Permalink to &quot;**Step 2: Retrieval**&quot;">​</a></h3><ul><li><p>Once data is indexed in the vector database, algorithms search and retrieve information fragments relevant to user prompts and queries. Frameworks like LangChain support many retrieval algorithms, including those based on semantic similarity, metadata, or parent documents.</p><p>In open-domain user settings, information retrieval comes from indexed documents on the internet, accessed through APIs from these information sources. In closed-domain enterprise settings, information needs to remain confidential and unaffected by external sources; RAG enables local retrieval and enhanced security.</p><p>Finally, the retrieved data is injected into the prompt and sent to the LLM for processing.</p></li></ul><h3 id="step-3-generation" tabindex="-1"><strong>Step 3: Generation</strong> <a class="header-anchor" href="#step-3-generation" aria-label="Permalink to &quot;**Step 3: Generation**&quot;">​</a></h3><ul><li><strong>Output:</strong> The response is delivered to the user. If RAG works as intended, the user receives an accurate answer based on the provided source knowledge.</li></ul><h2 id="_3-how-to-implement" tabindex="-1"><strong>3. How to Implement</strong> <a class="header-anchor" href="#_3-how-to-implement" aria-label="Permalink to &quot;**3. How to Implement**&quot;">​</a></h2><ol><li><p><strong>Knowledge Base Construction Phase</strong></p><ul><li>Data Source Assessment: Coverage, quality, and update frequency</li><li>Metadata Design: Source, timeliness, and authority tags</li><li>Version Management: Incremental updates and rollback mechanisms</li></ul></li><li><p><strong>System Integration Phase</strong></p><ul><li>Retrieval Strategy Configuration: <ul><li>Single-pass retrieval vs. multi-turn refinement</li><li>Exact matching vs. semantic generalization</li></ul></li><li>Generation Parameter Tuning: <ul><li>Citation formatting normalization</li><li>Terminology consistency maintenance</li></ul></li></ul></li><li><p><strong>Continuous Optimization Phase</strong></p><ul><li>Quality Monitoring Metrics: <ul><li>Retrieval hit rate</li><li>Factual accuracy of generation</li><li>User satisfaction</li></ul></li><li>Feedback Loop Design: <ul><li>Error case annotation</li><li>Automatic retraining triggers</li></ul></li></ul></li></ol><h2 id="_4-advantages" tabindex="-1"><strong>4. Advantages</strong> <a class="header-anchor" href="#_4-advantages" aria-label="Permalink to &quot;**4. Advantages**&quot;">​</a></h2><p>Because RAG has a built-in retrieval mechanism, it can leverage additional data sources beyond the conventional training set of the LLM. By grounding the LLM in a series of external, verifiable facts, RAG achieves several beneficial goals:</p><p><strong>Accuracy</strong><br> RAG provides the LLM with reference sources, enabling user verification. You can also design a RAG system to respond with &quot;I don&#39;t know&quot; when queries fall outside its knowledge scope. Overall, RAG can reduce the likelihood of the LLM sharing incorrect or misleading information, which in turn may increase user trust.</p><p><strong>Cost-Effectiveness</strong><br> Retraining and fine-tuning large language models is expensive and time-consuming, similar to building a <a href="https://www.redhat.com/en/topics/ai/what-are-foundation-models" target="_blank" rel="noreferrer">foundation model</a> from scratch and adding domain-specific information. With RAG, users can introduce new data into the LLM simply by uploading documents or files, making it easy to replace or update information sources.</p><p>RAG also reduces inference costs. LLM queries are costly—if you run the model locally, your own hardware must meet demanding requirements; if you use external APIs, you incur billing costs. RAG enables you to send only the most relevant portions of reference materials, rather than entire documents, reducing query size and improving efficiency.</p><p><strong>Developer Control</strong><br> Compared to traditional fine-tuning, RAG offers a more convenient and direct way to obtain feedback, troubleshoot, and fix applications. For developers, the major advantage of RAG is the ability to leverage up-to-date information streams from specific domains.</p><p><strong>Data Sovereignty and Privacy</strong><br> Using sensitive information to fine-tune LLMs has previously been risky, as LLMs might leak information from their training data. RAG allows sensitive data to remain local while still being used to inform local or trusted external LLMs, providing a solution for privacy concerns. RAG architectures can also restrict sensitive information retrieval to different authorization levels, so certain users can access information based on their security permissions.</p><h2 id="_5-application-scenarios" tabindex="-1"><strong>5. Application Scenarios</strong> <a class="header-anchor" href="#_5-application-scenarios" aria-label="Permalink to &quot;**5. Application Scenarios**&quot;">​</a></h2><table tabindex="0"><thead><tr><th style="text-align:center;">Scenario Type</th><th style="text-align:center;">Core Requirement</th><th style="text-align:center;">Key Technical Adaptation</th></tr></thead><tbody><tr><td style="text-align:center;">Enterprise Knowledge Base</td><td style="text-align:center;">Multi-source integration</td><td style="text-align:center;">Hierarchical access control</td></tr><tr><td style="text-align:center;">Intelligent Customer Service</td><td style="text-align:center;">Real-time accuracy</td><td style="text-align:center;">Dialogue state management</td></tr><tr><td style="text-align:center;">Academic Research</td><td style="text-align:center;">Source traceability</td><td style="text-align:center;">Professional terminology handling</td></tr><tr><td style="text-align:center;">Compliance Review</td><td style="text-align:center;">Clause matching accuracy</td><td style="text-align:center;">Legal validity assessment</td></tr></tbody></table><p>RAG architectures have many potential use cases. The most popular include:</p><ul><li><strong>Customer Service:</strong> Programming chatbots to answer customer inquiries using in-depth knowledge of specific documents, thus shortening resolution times and making customer support systems more efficient.</li><li><strong>Generating Insights:</strong> RAG can help query existing documents. By connecting LLMs to annual reports, marketing documents, social media comments, customer reviews, survey results, research documents, or other materials using RAG, you can find answers that help you better understand your resources. Note that RAG can connect directly to real-time data sources (such as social media feeds, websites, or other frequently updated resources) to generate up-to-date and practical answers.</li><li><strong>Medical Information Systems:</strong> RAG can improve systems that provide medical information or advice. It can review factors like personal medical history, appointment scheduling, and the latest research and guidelines, helping patients receive the support and services they need.</li></ul><h2 id="_6-solution-steps" tabindex="-1"><strong>6. Solution Steps</strong> <a class="header-anchor" href="#_6-solution-steps" aria-label="Permalink to &quot;**6. Solution Steps**&quot;">​</a></h2><ol><li>Create a RAG application</li><li>Build a document processing pipeline</li><li>Browse the large language model catalog</li><li>Select an appropriate model</li></ol><h2 id="_7-concept-clarification" tabindex="-1"><strong>7. Concept Clarification</strong> <a class="header-anchor" href="#_7-concept-clarification" aria-label="Permalink to &quot;**7. Concept Clarification**&quot;">​</a></h2><h3 id="_1-vector-database" tabindex="-1">1. Vector Database <a class="header-anchor" href="#_1-vector-database" aria-label="Permalink to &quot;1. Vector Database&quot;">​</a></h3><p>A vector database is a specialized database system for storing, retrieving, and managing vector embeddings. It plays a central role in applications that combine with large language models.</p><p>Traditional databases are not effective at handling high-dimensional vector similarity searches. Vector databases (such as Milvus, Pinecone, Weaviate) are optimized for these operations and support large-scale data.</p>',32)]))}const m=t(r,[["render",o]]);export{g as __pageData,m as default};
