<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Daci Blog</title>
    <meta name="description" content="Daci的博客">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.f-orAREn.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.DvBmvMPr.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.Dr5Fq-8O.js">
    <link rel="modulepreload" href="/assets/chunks/framework.BjlC_BXf.js">
    <link rel="modulepreload" href="/assets/en_ai_rag_rag4.md.OUTPwYaE.lean.js">
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="icon" type="image/svg+xml" href="/logo.svg">
    <meta name="theme-color" content="#3eaf7c">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-32e39dcf><!--[--><!--]--><!--[--><span tabindex="-1" data-v-b340c8f7></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-b340c8f7>Skip to content</a><!--]--><!----><header class="VPNav" data-v-32e39dcf data-v-298a4f5a><div class="VPNavBar" data-v-298a4f5a data-v-883acc06><div class="wrapper" data-v-883acc06><div class="container" data-v-883acc06><div class="title" data-v-883acc06><div class="VPNavBarTitle has-sidebar" data-v-883acc06 data-v-64c800ee><a class="title" href="/en/" data-v-64c800ee><!--[--><!--]--><!--[--><img class="VPImage logo" src="/logo.svg" alt data-v-2662aff2><!--]--><span data-v-64c800ee>Daci</span><!--[--><!--]--></a></div></div><div class="content" data-v-883acc06><div class="content-body" data-v-883acc06><!--[--><!--]--><div class="VPNavBarSearch search" data-v-883acc06><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-883acc06 data-v-0d30a5d7><span id="main-nav-aria-label" class="visually-hidden" data-v-0d30a5d7> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/en/" tabindex="0" data-v-0d30a5d7 data-v-fa9a032b><!--[--><span data-v-fa9a032b>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/en/ai/" tabindex="0" data-v-0d30a5d7 data-v-fa9a032b><!--[--><span data-v-fa9a032b>AI</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-0d30a5d7 data-v-f1032559><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-f1032559><span class="text" data-v-f1032559><!----><span data-v-f1032559>Tech</span><span class="vpi-chevron-down text-icon" data-v-f1032559></span></span></button><div class="menu" data-v-f1032559><div class="VPMenu" data-v-f1032559 data-v-2c3610af><div class="items" data-v-2c3610af><!--[--><!--[--><div class="VPMenuLink" data-v-2c3610af data-v-c8d6d8c5><a class="VPLink link" href="/en/tech/frontend/" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>Frontend</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-2c3610af data-v-c8d6d8c5><a class="VPLink link" href="/en/tech/backend/" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>Backend</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-0d30a5d7 data-v-f1032559><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-f1032559><span class="text" data-v-f1032559><!----><span data-v-f1032559>Personal project</span><span class="vpi-chevron-down text-icon" data-v-f1032559></span></span></button><div class="menu" data-v-f1032559><div class="VPMenu" data-v-f1032559 data-v-2c3610af><div class="items" data-v-2c3610af><!--[--><!--[--><div class="VPMenuLink" data-v-2c3610af data-v-c8d6d8c5><a class="VPLink link vp-external-link-icon" href="http://122.51.102.153:10012/cesium.html" target="_blank" rel="noreferrer" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>Cesium</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-2c3610af data-v-c8d6d8c5><a class="VPLink link vp-external-link-icon" href="http://122.51.102.153:10010/" target="_blank" rel="noreferrer" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>LandAI</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/en/tools/" tabindex="0" data-v-0d30a5d7 data-v-fa9a032b><!--[--><span data-v-fa9a032b>Tools</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-0d30a5d7 data-v-f1032559><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-f1032559><span class="text" data-v-f1032559><!----><span data-v-f1032559>About</span><span class="vpi-chevron-down text-icon" data-v-f1032559></span></span></button><div class="menu" data-v-f1032559><div class="VPMenu" data-v-f1032559 data-v-2c3610af><div class="items" data-v-2c3610af><!--[--><!--[--><div class="VPMenuLink" data-v-2c3610af data-v-c8d6d8c5><a class="VPLink link" href="/en/about/fitness/" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>Fitness</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-2c3610af data-v-c8d6d8c5><a class="VPLink link" href="/en/about/life/" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>Life</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-883acc06 data-v-aba76a3d data-v-f1032559><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Change language" data-v-f1032559><span class="text" data-v-f1032559><span class="vpi-languages option-icon" data-v-f1032559></span><!----><span class="vpi-chevron-down text-icon" data-v-f1032559></span></span></button><div class="menu" data-v-f1032559><div class="VPMenu" data-v-f1032559 data-v-2c3610af><!----><!--[--><!--[--><div class="items" data-v-aba76a3d><p class="title" data-v-aba76a3d>English</p><!--[--><div class="VPMenuLink" data-v-aba76a3d data-v-c8d6d8c5><a class="VPLink link" href="/ai/rag/rag4.html" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>简体中文</span><!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-883acc06 data-v-be3201b0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-be3201b0 data-v-23920a8f data-v-222e4e54><span class="check" data-v-222e4e54><span class="icon" data-v-222e4e54><!--[--><span class="vpi-sun sun" data-v-23920a8f></span><span class="vpi-moon moon" data-v-23920a8f></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-883acc06 data-v-ab7c40f2 data-v-5480e48c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/zhangdanqing/zhangdanqing.github.io" aria-label="github" target="_blank" rel="noopener" data-v-5480e48c data-v-1dcf205f><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-883acc06 data-v-b0498f15 data-v-f1032559><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-f1032559><span class="vpi-more-horizontal icon" data-v-f1032559></span></button><div class="menu" data-v-f1032559><div class="VPMenu" data-v-f1032559 data-v-2c3610af><!----><!--[--><!--[--><div class="group translations" data-v-b0498f15><p class="trans-title" data-v-b0498f15>English</p><!--[--><div class="VPMenuLink" data-v-b0498f15 data-v-c8d6d8c5><a class="VPLink link" href="/ai/rag/rag4.html" data-v-c8d6d8c5><!--[--><span data-v-c8d6d8c5>简体中文</span><!--]--></a></div><!--]--></div><div class="group" data-v-b0498f15><div class="item appearance" data-v-b0498f15><p class="label" data-v-b0498f15>Appearance</p><div class="appearance-action" data-v-b0498f15><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-b0498f15 data-v-23920a8f data-v-222e4e54><span class="check" data-v-222e4e54><span class="icon" data-v-222e4e54><!--[--><span class="vpi-sun sun" data-v-23920a8f></span><span class="vpi-moon moon" data-v-23920a8f></span><!--]--></span></span></button></div></div></div><div class="group" data-v-b0498f15><div class="item social-links" data-v-b0498f15><div class="VPSocialLinks social-links-list" data-v-b0498f15 data-v-5480e48c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/zhangdanqing/zhangdanqing.github.io" aria-label="github" target="_blank" rel="noopener" data-v-5480e48c data-v-1dcf205f><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-883acc06 data-v-41ff4f54><span class="container" data-v-41ff4f54><span class="top" data-v-41ff4f54></span><span class="middle" data-v-41ff4f54></span><span class="bottom" data-v-41ff4f54></span></span></button></div></div></div></div><div class="divider" data-v-883acc06><div class="divider-line" data-v-883acc06></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-32e39dcf data-v-14b7a3aa><div class="container" data-v-14b7a3aa><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-14b7a3aa><span class="vpi-align-left menu-icon" data-v-14b7a3aa></span><span class="menu-text" data-v-14b7a3aa>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-14b7a3aa data-v-cd011971><button data-v-cd011971>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-32e39dcf data-v-7f452ddf><div class="curtain" data-v-7f452ddf></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-7f452ddf><span class="visually-hidden" id="sidebar-aria-label" data-v-7f452ddf> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-a7d3a93e><section class="VPSidebarItem level-0" data-v-a7d3a93e data-v-48f9c2c2><div class="item" role="button" tabindex="0" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><h2 class="text" data-v-48f9c2c2>AI</h2><!----></div><div class="items" data-v-48f9c2c2><!--[--><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/en/ai/" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>AI Tools Guide</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/en/ai/cursor.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>AI Applications in Cursor</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/en/ai/windsurf.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>Exploring AI in Windsurf Editor</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/en/ai/rag/rag1.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>RAG Analysis</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/en/ai/rag/rag2.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>Analysis of Large Model Application Framework: RAG, Agent, Fine tuning, Prompt Word Engineering</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/en/ai/rag/rag3.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>Quickly build a RAG Q&A system that supports multimodal documents</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-48f9c2c2 data-v-48f9c2c2><div class="item" data-v-48f9c2c2><div class="indicator" data-v-48f9c2c2></div><a class="VPLink link link" href="/en/ai/rag/rag4.html" data-v-48f9c2c2><!--[--><p class="text" data-v-48f9c2c2>RAG text segmentation, from strategy to optimization</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-32e39dcf data-v-82c0ecc6><div class="VPDoc has-sidebar has-aside" data-v-82c0ecc6 data-v-06ef2308><!--[--><!--]--><div class="container" data-v-06ef2308><div class="aside" data-v-06ef2308><div class="aside-curtain" data-v-06ef2308></div><div class="aside-container" data-v-06ef2308><div class="aside-content" data-v-06ef2308><div class="VPDocAside" data-v-06ef2308 data-v-75b22be8><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-75b22be8 data-v-a2152973><div class="content" data-v-a2152973><div class="outline-marker" data-v-a2152973></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a2152973>On this page</div><ul class="VPDocOutlineItem root" data-v-a2152973 data-v-80416021><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-75b22be8></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-06ef2308><div class="content-container" data-v-06ef2308><!--[--><!--]--><main class="main" data-v-06ef2308><div style="position:relative;" class="vp-doc _en_ai_rag_rag4" data-v-06ef2308><div><h2 id="abstract" tabindex="-1">Abstract <a class="header-anchor" href="#abstract" aria-label="Permalink to &quot;Abstract&quot;">​</a></h2><p>The performance of Retrieval-Augmented Generation (RAG) largely depends on the quality of its retrieval module, with text chunking being the key prerequisite for effective retrieval. Crude or inappropriate chunking can lead to information loss, context fragmentation, and low retrieval efficiency, severely impacting the final performance of RAG systems. This article systematically explores the core objectives and challenges of text chunking, highlighting its importance from the perspectives of overcoming context window limitations, improving retrieval accuracy, and maintaining contextual integrity. It then details foundational chunking strategies such as fixed-size, sentence-based, recursive character, and document-structure-based chunking, and provides a Python implementation of a hybrid chunking strategy utilizing Markdown document structure. Furthermore, the article discusses advanced strategies like semantic chunking, hierarchical chunking, and sentence window retrieval, offering readers a broader technical perspective. Mastering sophisticated text chunking techniques is a critical step toward abandoning “crude splitting” and building efficient, precise RAG applications.</p><h2 id="preface" tabindex="-1">Preface <a class="header-anchor" href="#preface" aria-label="Permalink to &quot;Preface&quot;">​</a></h2><p>Retrieval-Augmented Generation (RAG) combines external knowledge bases with the generative power of large language models (LLMs), significantly enhancing the accuracy and timeliness of model responses. This technology has become a popular choice for building intelligent Q&amp;A and content generation applications. However, behind the scenes of shiny RAG applications, a usually overlooked but crucial step is how to <strong>split massive raw documents into suitable &quot;chunks&quot;</strong> for retrieval.</p><p>Many developers, when building RAG systems, may opt for the simplest method, such as splitting by a fixed number of characters or paragraphs. While seemingly convenient, such “brute-force” chunking often becomes the primary culprit for underperforming or “dumb” RAG systems. Imagine:</p><ol><li><strong>Context “Severed”</strong>: A complete logical chain, an important code example, or a key definition gets cut in the middle. The retrieved chunk contains only partial information, making it difficult for the LLM to provide a complete and accurate answer.</li><li><strong>Key Information “Sinks”</strong>: Important information happens to be scattered across chunk boundaries, resulting in fragmented retrieval or outright omission.</li><li><strong>Retrieval “Lost”</strong>: Oversized chunks contain too much noise, reducing matching accuracy; undersized chunks lack semantic depth, also harming retrieval. Efficiency plummets, and many “apparently relevant but actually useless” results are returned.</li></ol><p>Choosing and designing suitable text chunking strategies is thus no trivial matter. It is akin to laying the foundation for the information highway of a RAG system, directly determining whether the retriever can accurately and efficiently locate information, and thus whether the LLM can generate satisfying answers based on high-quality context.</p><h2 id="i-why-is-text-chunking-essential-for-rag" tabindex="-1">I. Why Is Text Chunking Essential for RAG? <a class="header-anchor" href="#i-why-is-text-chunking-essential-for-rag" aria-label="Permalink to &quot;I. Why Is Text Chunking Essential for RAG?&quot;">​</a></h2><p>Before delving into chunking strategies, it’s important to clarify: Why is text chunking indispensable in RAG architectures? What core problems does it address?</p><h3 id="_1-1-definition-and-core-objectives" tabindex="-1">1.1 Definition and Core Objectives <a class="header-anchor" href="#_1-1-definition-and-core-objectives" aria-label="Permalink to &quot;1.1 Definition and Core Objectives&quot;">​</a></h3><p><strong>Text chunking (Text Splitting)</strong> refers to the process of breaking down large raw text (e.g., long reports, ebooks, complex web pages, or extensive API docs) into a series of smaller, more manageable text fragments (“chunks”). These chunks are the basic units for information processing in RAG systems—they get vectorized via embedding models and indexed in vector databases for retrieval.</p><p>The core objectives of text chunking can be summarized as follows:</p><ol><li><strong>Overcoming Context Window Limitations</strong>: All LLMs, whether GPT-series, Claude, or Llama, have limitations on the amount of context they can process at once. Raw documents often far exceed these limits (thousands to hundreds of thousands of tokens). Without chunking, it’s impossible to provide the LLM with the full document. Chunking ensures that each input segment is within the LLM’s “digestible” range.</li><li><strong>Improving Retrieval Accuracy and Efficiency</strong>: <ul><li><strong>Accuracy</strong>: For queries like “What is the self-attention mechanism in Transformer?”, a precisely chunked segment containing the definition (a few hundred words) is easier for similarity search to hit than a chunk containing the entire paper. Large chunks dilute key signals; <strong>small, semantically focused chunks enable precise matching</strong>.</li><li><strong>Efficiency</strong>: Vectorizing and indexing many small chunks makes vector database retrieval much faster than full-text search across entire documents. Chunking serves as effective preprocessing, enabling rapid pinpointing of the most relevant fragments.</li></ul></li><li><strong>Maintaining Contextual Integrity</strong>: While splitting is necessary, the ideal chunking strategy <strong>should preserve semantic coherence and context</strong>—never splitting mid-sentence, code block, or list item. Each chunk should ideally carry a relatively complete and meaningful information unit.</li></ol><h3 id="_1-2-profound-impact-on-rag-performance" tabindex="-1">1.2 Profound Impact on RAG Performance <a class="header-anchor" href="#_1-2-profound-impact-on-rag-performance" aria-label="Permalink to &quot;1.2 Profound Impact on RAG Performance&quot;">​</a></h3><p>Understanding these goals clarifies how chunking directly impacts RAG system performance. The chosen strategy acts like fitting the RAG system with different “information pipelines,” determining the quality and efficiency of information flow. This affects two core stages:</p><ul><li><p><strong>Retrieval Quality</strong>: The chunk size (granularity), boundary determination (by sentence? paragraph? fixed size?), and overlap rules directly affect whether the retriever can accurately and comprehensively find the most relevant text fragments. Poor chunking yields:</p><ul><li><strong>Irrelevant information</strong>: Chunks too large, with excessive noise.</li><li><strong>Incomplete information</strong>: Chunks too small or cut inappropriately, losing context.</li><li><strong>Redundant information</strong>: Multiple chunks with high overlap or similar content.</li></ul></li><li><p><strong>Generation Quality</strong>: The generator (LLM) relies on the context provided by the retriever (the retrieved chunks). If chunks are of low quality (fragmented context, missing information, critical omissions), even the strongest LLM “cannot make bricks without straw”—making it hard to generate accurate, fluent, and logically coherent answers. <strong>High-quality retrieval is a prerequisite for high-quality generation, and appropriate chunking is the foundation of high-quality retrieval</strong>.</p></li></ul><h3 id="_1-3-intrinsic-trade-off-precision-vs-context" tabindex="-1">1.3 Intrinsic Trade-off: Precision vs. Context <a class="header-anchor" href="#_1-3-intrinsic-trade-off-precision-vs-context" aria-label="Permalink to &quot;1.3 Intrinsic Trade-off: Precision vs. Context&quot;">​</a></h3><p>Chunking involves a classic trade-off: <strong>retrieval precision</strong> versus <strong>contextual completeness</strong>.</p><ul><li><p><strong>Smaller Chunks</strong>:</p><ul><li><strong>Pros</strong>: More focused information, higher semantic concentration. Query vectors are more likely to match chunks with specific keywords or highly relevant semantics—improving retrieval precision.</li><li><strong>Cons</strong>: May lose important context. A small chunk may only contain a fact fragment, lacking necessary background or follow-up explanations—making comprehensive answers difficult for the LLM.</li></ul></li><li><p><strong>Larger Chunks</strong>:</p><ul><li><strong>Pros</strong>: Retain richer contextual information, enabling understanding of complex concepts or relationships.</li><li><strong>Cons</strong>: May contain much unrelated information (noise), diluting core signals and reducing retrieval precision. Also increases the LLM’s processing load and potential for hallucinations.</li></ul></li></ul><p>Thus, <strong>choosing or designing a chunking strategy hinges on finding the optimal balance for your data (text type, structure, information density) and application scenario (QA, summarization, dialogue, etc.)</strong>. No single strategy is universally optimal—understanding this trade-off is foundational.</p><h2 id="ii-basic-chunking-strategies" tabindex="-1">II. Basic Chunking Strategies <a class="header-anchor" href="#ii-basic-chunking-strategies" aria-label="Permalink to &quot;II. Basic Chunking Strategies&quot;">​</a></h2><p>Familiarity with some fundamental chunking strategies is the starting point for building RAG systems. Each has pros and cons and is suitable for different scenarios.</p><h3 id="_2-1-fixed-size-chunking" tabindex="-1">2.1 Fixed-Size Chunking <a class="header-anchor" href="#_2-1-fixed-size-chunking" aria-label="Permalink to &quot;2.1 Fixed-Size Chunking&quot;">​</a></h3><p>The simplest “lazy” method: split text by a fixed character or token count, often with a set “overlap” to help preserve context at boundaries.</p><ul><li><strong>Core Idea</strong>: Set <code>chunk_size</code> (e.g., 500 characters) and <code>chunk_overlap</code> (e.g., 50 characters). Move through the text, chunking accordingly.</li><li><strong>Pros</strong>: <ul><li>Easy to implement.</li><li>Low computational overhead.</li><li>No special text format requirements.</li></ul></li><li><strong>Cons</strong>: <ul><li><strong>Likely to break semantic integrity</strong> (may cut in the middle of sentences, code, words).</li><li><strong>Ignores text structure</strong> (paragraphs, headings, lists, etc.).</li><li>The same chunk size may contain very different amounts of information in dense vs. sparse text.</li></ul></li><li><strong>Use Cases</strong>: <ul><li>Simple scenarios not requiring structural awareness.</li><li>Large-scale data needing rapid preliminary processing.</li><li>As a fallback for more complex strategies.</li></ul></li></ul><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Conceptual example (not for direct execution)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> fixed_size_chunking</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(text, chunk_size, chunk_overlap):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    chunks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    start_index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    while</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> start_index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(text):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        end_index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> start_index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> chunk_size</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        chunks.append(text[start_index:end_index])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        start_index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> chunk_size </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> chunk_overlap</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> start_index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(text):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">            break</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> chunks</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># chunks = fixed_size_chunking(text, 500, 50)</span></span></code></pre></div><h3 id="_2-2-sentence-based-chunking" tabindex="-1">2.2 Sentence-Based Chunking <a class="header-anchor" href="#_2-2-sentence-based-chunking" aria-label="Permalink to &quot;2.2 Sentence-Based Chunking&quot;">​</a></h3><p>This strategy aims to respect natural language boundaries by splitting first by sentence (using punctuation or NLP libraries like NLTK or SpaCy), then combining sentences into chunks up to a target size.</p><ul><li><strong>Core Idea</strong>: Split into sentences, then group consecutive sentences into chunks, possibly with sentence-level overlap.</li><li><strong>Pros</strong>: <ul><li><strong>Preserves semantic integrity</strong> (sentences are basic semantic units).</li><li>More natural than fixed-size chunking.</li></ul></li><li><strong>Cons</strong>: <ul><li><strong>Sentence length varies</strong> (chunk sizes uneven).</li><li>Punctuation-based splitting can be inaccurate (e.g., “Mr. Smith”).</li><li>Not suitable for code, lists, or text without clear sentences.</li><li>May still split complex semantic relationships across chunks.</li></ul></li><li><strong>Use Cases</strong>: <ul><li>Well-structured, sentence-based text (news, reports, novels).</li><li>When sentence-level semantic integrity is important.</li></ul></li></ul><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Conceptual example (simple punctuation splitting)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> re</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> sentence_chunking</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(text, max_chunk_sentences</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    sentences </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> re.split(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">r</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">(?&lt;=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">[.?!]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">)</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\s</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, text)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    sentences </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [s </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> s </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sentences </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> s]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    chunks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    current_chunk_sentences </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sentence </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sentences:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        current_chunk_sentences.append(sentence)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(current_chunk_sentences) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> max_chunk_sentences:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            chunks.append(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot; &quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.join(current_chunk_sentences))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            current_chunk_sentences </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> current_chunk_sentences:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        chunks.append(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot; &quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.join(current_chunk_sentences))</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> chunks</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># chunks = sentence_chunking(text, 3)</span></span></code></pre></div><h3 id="_2-3-recursive-character-splitting" tabindex="-1">2.3 Recursive Character Splitting <a class="header-anchor" href="#_2-3-recursive-character-splitting" aria-label="Permalink to &quot;2.3 Recursive Character Splitting&quot;">​</a></h3><p>Popular in frameworks like LangChain, this strategy tries to split text by a prioritized list of delimiters (e.g., paragraphs, lines, spaces, then characters).</p><ul><li><strong>Core Idea</strong>: Attempt to split by <code>\n\n</code> (paragraph), then <code>\n</code> (line), then spaces, then characters, in priority order, ensuring chunks do not exceed size limits.</li><li><strong>Pros</strong>: <ul><li>Tries to preserve semantic structure where possible.</li><li>Smarter than fixed-size chunking.</li><li>Adaptive to various text types.</li></ul></li><li><strong>Cons</strong>: <ul><li>More complex to implement.</li><li>Effectiveness depends on delimiter choice and order.</li><li>May degrade to character splitting for dense text.</li></ul></li><li><strong>Use Cases</strong>: <ul><li>General purpose, suitable for many document types.</li><li>When both structure and size control are desired.</li><li>Default/recommended in many RAG frameworks.</li></ul></li></ul><h3 id="_2-4-document-structure-aware-chunking" tabindex="-1">2.4 Document Structure-Aware Chunking <a class="header-anchor" href="#_2-4-document-structure-aware-chunking" aria-label="Permalink to &quot;2.4 Document Structure-Aware Chunking&quot;">​</a></h3><p>This strategy splits based on the inherent structure of documents—HTML tags, Markdown headings, lists, or JSON/YAML hierarchy.</p><ul><li><strong>Core Idea</strong>: Parse the document’s structure (e.g., each <code>&lt;p&gt;</code> tag or Markdown heading and its content as a chunk).</li><li><strong>Pros</strong>: <ul><li><strong>Highly respects original structure</strong>.</li><li><strong>Strong semantic coherence</strong>.</li><li>Facilitates attaching metadata (headings/tags) to chunks.</li></ul></li><li><strong>Cons</strong>: <ul><li><strong>Requires clear, consistent structure</strong>.</li><li>Chunks may vary greatly in size.</li><li>Different formats need different parsing logic.</li></ul></li><li><strong>Use Cases</strong>: <ul><li>Well-structured documents (web pages, Markdown, structured data).</li><li>When leveraging document structure for retrieval/filtering.</li></ul></li></ul><h3 id="_2-5-hybrid-chunking" tabindex="-1">2.5 Hybrid Chunking <a class="header-anchor" href="#_2-5-hybrid-chunking" aria-label="Permalink to &quot;2.5 Hybrid Chunking&quot;">​</a></h3><p>Combines the strengths of multiple methods. A common approach is to first split by structure (e.g., Markdown headings), then further split oversized segments using recursive or sentence-based methods.</p><ul><li><strong>Core Idea</strong>: Layered processing—first coarse-grained (structure/semantic), then fine-grained (size control), retaining context and metadata.</li><li><strong>Pros</strong>: <ul><li><strong>Balances structure and size</strong>.</li><li><strong>Rich metadata</strong>.</li><li>Highly flexible and customizable.</li></ul></li><li><strong>Cons</strong>: <ul><li>More complex implementation.</li><li>Requires careful design of logic and parameters.</li></ul></li><li><strong>Use Cases</strong>: <ul><li>High chunk quality demands—balancing context, structure, and size.</li><li>Structured yet free-form content (Markdown, rich text).</li></ul></li></ul><h2 id="iii-advanced-chunking-strategies" tabindex="-1">III. Advanced Chunking Strategies <a class="header-anchor" href="#iii-advanced-chunking-strategies" aria-label="Permalink to &quot;III. Advanced Chunking Strategies&quot;">​</a></h2><p>For more complex needs or to maximize retrieval performance, consider advanced chunking methods that focus on <strong>semantic understanding</strong> or use more sophisticated models/processes.</p><h3 id="_3-1-semantic-chunking" tabindex="-1">3.1 Semantic Chunking <a class="header-anchor" href="#_3-1-semantic-chunking" aria-label="Permalink to &quot;3.1 Semantic Chunking&quot;">​</a></h3><ul><li><strong>Core Idea</strong>: Instead of counting characters or looking at punctuation, segment text based on meaning—e.g., compute embedding vectors for adjacent sentences or small text units, and split at “semantic breakpoints” where topic shifts (vector similarity below a threshold).</li><li><strong>Pros</strong>: Chunks are semantically coherent—splits occur at topic transitions, yielding segments better aligned with human understanding.</li><li><strong>Cons</strong>: Requires embedding computation (costly); effectiveness depends on the embedding model and threshold selection; slower processing.</li><li><strong>Use Cases</strong>: High chunk quality requirements and adequate resources, especially for unstructured yet content-rich text.</li></ul><blockquote><p><strong>Example Approach</strong>: Group several sentences (e.g., 3) as a “semantic unit,” compute embeddings, and compare adjacent unit similarities—split where similarity drops. Effectiveness requires experimentation.</p></blockquote><h3 id="_3-2-hierarchical-chunking" tabindex="-1">3.2 Hierarchical Chunking <a class="header-anchor" href="#_3-2-hierarchical-chunking" aria-label="Permalink to &quot;3.2 Hierarchical Chunking&quot;">​</a></h3><ul><li><strong>Core Idea</strong>: Systematically create multiple <strong>levels</strong> of chunks—e.g., split by chapters (L1), paragraphs within chapters (L2), sentences within paragraphs (L3)—with each level indexed for different retrieval strategies (see Small-to-Big).</li><li><strong>Pros</strong>: Provides context at various granularity, increasing retrieval flexibility.</li><li><strong>Cons</strong>: Greater indexing and storage complexity; needs careful hierarchy design.</li><li><strong>Use Cases</strong>: Complex documents with clear hierarchy (books, reports), needing flexible context selection.</li></ul><h3 id="_3-3-small-to-big-retrieval-parent-document-retriever" tabindex="-1">3.3 Small-to-Big Retrieval / Parent Document Retriever <a class="header-anchor" href="#_3-3-small-to-big-retrieval-parent-document-retriever" aria-label="Permalink to &quot;3.3 Small-to-Big Retrieval / Parent Document Retriever&quot;">​</a></h3><ul><li><strong>Core Idea</strong>: Not strictly a chunking but a <strong>retrieval strategy</strong> based on hierarchical or parent-child chunks: <ol><li>Split documents into <strong>small chunks</strong> (for precise vector retrieval).</li><li>Maintain mappings to their <strong>parent chunks</strong> (e.g., sentence → paragraph).</li><li>Retrieve using small chunks.</li><li>Return the parent chunk to the LLM instead of the small chunk.</li></ol></li><li><strong>Pros</strong>: Combines <strong>precision</strong> of small-chunk retrieval with <strong>context</strong> of larger chunks—pinpoint accuracy with richer background.</li><li><strong>Cons</strong>: Requires maintaining mappings; increased complexity.</li><li><strong>Use Cases</strong>: RAG applications needing both high-precision retrieval and ample context.</li></ul><h3 id="_3-4-proposition-chunking" tabindex="-1">3.4 Proposition Chunking <a class="header-anchor" href="#_3-4-proposition-chunking" aria-label="Permalink to &quot;3.4 Proposition Chunking&quot;">​</a></h3><ul><li><strong>Core Idea</strong>: Decompose text into atomic <strong>propositions</strong> (facts or claims), often using LLMs or NLP models to extract these from sentences. For example, “Apple released the Vision Pro headset in 2023, priced at $3499” might be broken into “Apple released Vision Pro,” “Vision Pro released in 2023,” “Vision Pro priced at $3499.”</li><li><strong>Pros</strong>: Yields highly granular, focused knowledge units—ideal for fact-based QA.</li><li><strong>Cons</strong>: <ul><li>Highly dependent on LLM/NLP extraction quality.</li><li>Computationally expensive and slow.</li><li>May lose nuance, tone, and complex relationships.</li><li>Combining retrieved propositions for generation is challenging.</li></ul></li><li><strong>Use Cases</strong>: Knowledge base construction, fact-based QA, high-precision needs.</li></ul><h3 id="_3-5-agentic-llm-based-chunking" tabindex="-1">3.5 Agentic / LLM-Based Chunking <a class="header-anchor" href="#_3-5-agentic-llm-based-chunking" aria-label="Permalink to &quot;3.5 Agentic / LLM-Based Chunking&quot;">​</a></h3><ul><li><strong>Core Idea</strong>: Employ an <strong>agent</strong> or LLM to <strong>decide chunk boundaries</strong>—using prompts for content understanding, or dynamically selecting/composing chunking strategies.</li><li><strong>Pros</strong>: Potentially the most intelligent, context- and meaning-aware chunking.</li><li><strong>Cons</strong>: <ul><li>Highly complex, requires advanced prompting or agent logic.</li><li>Costly (LLM call overhead), slow.</li><li>Less deterministic and stable.</li><li>Still experimental; maturity and reliability are evolving.</li></ul></li><li><strong>Use Cases</strong>: Research projects, or applications requiring the utmost chunk quality regardless of cost.</li></ul><h2 id="iv-chunk-optimization-strategies" tabindex="-1">IV. Chunk Optimization Strategies <a class="header-anchor" href="#iv-chunk-optimization-strategies" aria-label="Permalink to &quot;IV. Chunk Optimization Strategies&quot;">​</a></h2><p>✨ <strong>Context Enrichment</strong> (Supplementary Strategy)</p><ul><li><strong>Core Idea</strong>: Not an independent chunking method, but after chunking, <strong>add extra contextual information</strong> to each chunk—e.g., adjacent sentences or summary info, or the parent section/heading as metadata (as in MarkdownHeaderTextSplitter).</li><li><strong>Pros</strong>: Provides more clues to the LLM without greatly increasing chunk size, helping it understand the chunk’s position and background.</li><li><strong>Cons</strong>: Requires additional processing to extract and attach enrichment info.</li><li><strong>Use Cases</strong>: Can be combined with any chunking strategy—especially useful when chunks are small and context may be lacking.</li></ul><p><strong>Summary</strong>: Advanced strategies offer finer-grained and more intelligent chunking, but at the cost of greater complexity and computation. Choosing a strategy requires careful weighing of effect, cost, and implementation complexity in the specific application context.</p></div></div></main><footer class="VPDocFooter" data-v-06ef2308 data-v-c461bbcf><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-c461bbcf><span class="visually-hidden" id="doc-footer-aria-label" data-v-c461bbcf>Pager</span><div class="pager" data-v-c461bbcf><!----></div><div class="pager" data-v-c461bbcf><a class="VPLink link pager-link next" href="/en/ai/" data-v-c461bbcf><!--[--><span class="desc" data-v-c461bbcf>Next page</span><span class="title" data-v-c461bbcf>AI Tools Guide</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-32e39dcf data-v-5458bf99><div class="container" data-v-5458bf99><p class="message" data-v-5458bf99>用心记录，温暖分享</p><p class="copyright" data-v-5458bf99> 2024 Daci Blog</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_fitness_index.md\":\"Bou26D0Q\",\"about_fitness_training_warmup1.md\":\"DaJAcpIR\",\"about_fitness_training_warmup2.md\":\"CW0x0uvD\",\"about_life_index.md\":\"CXbzT0g_\",\"about_life_thoughts_growth-mindset.md\":\"DothoJZu\",\"ai_cursor.md\":\"DoIxipQl\",\"ai_index.md\":\"BXdtWxVV\",\"ai_rag_rag1.md\":\"DcqMX1H9\",\"ai_rag_rag2.md\":\"u6NiAyZc\",\"ai_rag_rag3.md\":\"CQNjp5Y9\",\"ai_rag_rag4.md\":\"D9UiMew5\",\"ai_windsurf.md\":\"rI8GWiq6\",\"en_about_fitness_index.md\":\"CR4hSf6R\",\"en_about_fitness_training_warmup1.md\":\"CL_G46HX\",\"en_about_fitness_training_warmup2.md\":\"BH_bcG88\",\"en_about_life_index.md\":\"chI4W3Wr\",\"en_about_life_thoughts_growth-mindset.md\":\"ByVm340J\",\"en_ai_cursor.md\":\"BKBwgDf3\",\"en_ai_index.md\":\"C_fO7ew3\",\"en_ai_rag_rag1.md\":\"BgQfIkUI\",\"en_ai_rag_rag2.md\":\"BKIpR57u\",\"en_ai_rag_rag3.md\":\"WiNxM9uH\",\"en_ai_rag_rag4.md\":\"OUTPwYaE\",\"en_ai_windsurf.md\":\"C_QNWWj4\",\"en_index.md\":\"CMKipQKC\",\"en_markdown-examples.md\":\"Bur12mIb\",\"en_tech_backend_index.md\":\"ltmUyQ_J\",\"en_tech_frontend_20250307.md\":\"DlVZMUfG\",\"en_tech_frontend_index.md\":\"DIJIjZrz\",\"en_tech_ops_docker_basic-usage.md\":\"yV3omPMd\",\"en_tech_ops_index.md\":\"CXfV4Buh\",\"en_tools_index.md\":\"BizG2mWx\",\"index.md\":\"BZUE4Mp3\",\"tech_backend_index.md\":\"BzUtPg_W\",\"tech_frontend_20250307.md\":\"ZBu71huP\",\"tech_frontend_index.md\":\"C4U-7EDh\",\"tech_frontend_javascript-basics.md\":\"CQqAw1sd\",\"tech_ops_docker_basic-usage.md\":\"DkYSGKfn\",\"tech_ops_index.md\":\"Ca3w2Wmr\",\"tools_index.md\":\"Cx0LaOrB\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"Daci Blog\",\"description\":\"Daci的博客\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"siteTitle\":\"Daci\",\"logo\":\"/logo.svg\",\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/zhangdanqing/zhangdanqing.github.io\"}],\"footer\":{\"message\":\"用心记录，温暖分享\",\"copyright\":\" 2024 Daci Blog\"}},\"locales\":{\"root\":{\"label\":\"简体中文\",\"lang\":\"zh-CN\",\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"AI\",\"link\":\"/ai/\"},{\"text\":\"技术\",\"items\":[{\"text\":\"前端\",\"link\":\"/tech/frontend/\"},{\"text\":\"后端\",\"link\":\"/tech/backend/\"}]},{\"text\":\"个人项目\",\"items\":[{\"text\":\"Cesium\",\"link\":\"http://122.51.102.153:10012/cesium.html\"},{\"text\":\"LandAI\",\"link\":\"http://122.51.102.153:10010/\"}]},{\"text\":\"工具\",\"link\":\"/tools/\"},{\"text\":\"关于\",\"items\":[{\"text\":\"健身\",\"link\":\"/about/fitness/\"},{\"text\":\"生活\",\"link\":\"/about/life/\"}]}],\"sidebar\":{\"/ai/\":[{\"text\":\"AI\",\"items\":[{\"text\":\"AI 导览\",\"link\":\"/ai/\"},{\"text\":\"Cursor中AI的应用\",\"link\":\"/ai/cursor\"},{\"text\":\"深度探索Windsurf编辑器中AI的应用\",\"link\":\"/ai/windsurf\"},{\"text\":\"解析RAG\",\"link\":\"/ai/rag/rag1\"},{\"text\":\"大模型应用框架解析：RAG、Agent、微调、提示词工程\",\"link\":\"/ai/rag/rag2\"},{\"text\":\"快速搭建支持多模态文档的 RAG 问答系统\",\"link\":\"/ai/rag/rag3\"},{\"text\":\"RAG 文本分块，从策略到优化\",\"link\":\"/ai/rag/rag4\"}]}],\"/tools/\":[{\"text\":\"工具\",\"items\":[{\"text\":\"工具导览\",\"link\":\"/tools/\"}]}],\"/tech/frontend/\":[{\"text\":\"前端开发\",\"collapsed\":false,\"items\":[{\"text\":\"Next.js全栈实战：手把手集成NextAuth实现Google/GitHub一键登录\",\"link\":\"/tech/frontend/20250307\"},{\"text\":\"NocoBase 用户指南\",\"link\":\"/tech/frontend/\"}]}],\"/tech/backend/\":[{\"text\":\"后端开发\",\"collapsed\":true,\"items\":[{\"text\":\"使用 NestJS 搭建后端项目\",\"link\":\"/tech/backend/\"}]}],\"/about/fitness/\":[{\"text\":\"健身\",\"items\":[{\"text\":\"背部训练-上背和肩后束\",\"link\":\"/about/fitness/training/warmUp2\"},{\"text\":\"上肢热身/纠正性训练\",\"link\":\"/about/fitness/training/warmUp1\"},{\"text\":\"肩袖肌群受伤、肩关节弹响\",\"link\":\"/about/fitness/\"}]}],\"/about/life/\":[{\"text\":\"生活\",\"items\":[{\"text\":\"关于生活与成长的思考\",\"link\":\"/about/life/\"}]}]}}},\"en\":{\"label\":\"English\",\"lang\":\"en-US\",\"themeConfig\":{\"nav\":[{\"text\":\"Home\",\"link\":\"/en/\"},{\"text\":\"AI\",\"link\":\"/en/ai/\"},{\"text\":\"Tech\",\"items\":[{\"text\":\"Frontend\",\"link\":\"/en/tech/frontend/\"},{\"text\":\"Backend\",\"link\":\"/en/tech/backend/\"}]},{\"text\":\"Personal project\",\"items\":[{\"text\":\"Cesium\",\"link\":\"http://122.51.102.153:10012/cesium.html\"},{\"text\":\"LandAI\",\"link\":\"http://122.51.102.153:10010/\"}]},{\"text\":\"Tools\",\"link\":\"/en/tools/\"},{\"text\":\"About\",\"items\":[{\"text\":\"Fitness\",\"link\":\"/en/about/fitness/\"},{\"text\":\"Life\",\"link\":\"/en/about/life/\"}]}],\"sidebar\":{\"/en/ai/\":[{\"text\":\"AI\",\"items\":[{\"text\":\"AI Tools Guide\",\"link\":\"/en/ai/\"},{\"text\":\"AI Applications in Cursor\",\"link\":\"/en/ai/cursor\"},{\"text\":\"Exploring AI in Windsurf Editor\",\"link\":\"/en/ai/windsurf\"},{\"text\":\"RAG Analysis\",\"link\":\"en/ai/rag/rag1\"},{\"text\":\"Analysis of Large Model Application Framework: RAG, Agent, Fine tuning, Prompt Word Engineering\",\"link\":\"en/ai/rag/rag2\"},{\"text\":\"Quickly build a RAG Q&A system that supports multimodal documents\",\"link\":\"en/ai/rag/rag3\"},{\"text\":\"RAG text segmentation, from strategy to optimization\",\"link\":\"en/ai/rag/rag4\"}]}],\"/en/tools/\":[{\"text\":\"Tools\",\"items\":[{\"text\":\"Tools Guide\",\"link\":\"/en/tools/\"}]}],\"/en/tech/frontend/\":[{\"text\":\"Frontend Development\",\"collapsed\":false,\"items\":[{\"text\":\"Next.js Full-Stack Practice: Step-by-Step Integration of NextAuth for Google/GitHub One-Click Login\",\"link\":\"/en/tech/frontend/20250307\"},{\"text\":\"NocoBase User Guide\",\"link\":\"/en/tech/frontend/\"}]}],\"/en/tech/backend/\":[{\"text\":\"Backend Development\",\"collapsed\":true,\"items\":[{\"text\":\"Building a Backend Project with NestJS\",\"link\":\"/en/tech/backend/\"}]}],\"/en/about/fitness/\":[{\"text\":\"Fitness\",\"items\":[{\"text\":\"Back Training - Upper Back and Rear Deltoid\",\"link\":\"/en/about/fitness/training/warmUp2\"},{\"text\":\"Upper limb warm-up/corrective training\",\"link\":\"/en/about/fitness/training/warmUp1\"},{\"text\":\"Shoulder muscle group injury, shoulder joint popping\",\"link\":\"/en/about/fitness/\"}]}],\"/en/about/life/\":[{\"text\":\"Life\",\"items\":[{\"text\":\"Thoughts on Personal Work and Life\",\"link\":\"/en/about/life/\"}]}]}}}},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>